{"cells":[{"cell_type":"markdown","metadata":{"id":"-aPZe9E5HBAy"},"source":["Scrapping using beautifulsoup"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24309,"status":"ok","timestamp":1696938180601,"user":{"displayName":"Rizky yanuar Kristianto","userId":"11223808917659739908"},"user_tz":-420},"id":"O5eG2jd18G-q","outputId":"39e6176b-e76a-472e-afec-8699be57fc1e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22292,"status":"ok","timestamp":1696940599483,"user":{"displayName":"Rizky yanuar Kristianto","userId":"11223808917659739908"},"user_tz":-420},"id":"BUZkOsjCDBvZ","outputId":"8a0d2c18-e218-4118-8a8e-7543958b1cec"},"outputs":[{"name":"stdout","output_type":"stream","text":["Masukkan kata kunci: speech to text \n","pilih urutan berdasarkan\n","ketik 1 untuk artikel yang relevan\n","ketik 0 untuk artikel yang terbaru :\n","1\n","Masukkan jumlah artikel yang dicari: 10\n"]}],"source":["import requests\n","from bs4 import BeautifulSoup\n","import csv\n","\n","# Kata kunci pencarian\n","kata_kunci = input(\"Masukkan kata kunci: \")\n","kata_kunci = kata_kunci.replace(' ', '+')\n","\n","sort_input = int(input('''pilih urutan berdasarkan\n","ketik 1 untuk artikel yang relevan\n","ketik 0 untuk artikel yang terbaru :\n","'''))\n","\n","# Atur parameter urutan berdasarkan input pengguna\n","sort = 'Relevance' if sort_input == 1 else 'PubDate'\n","\n","# Jumlah baris maksimum\n","max_rows = int(input(\"Masukkan jumlah artikel yang dicari: \"))\n","\n","# Membuka file CSV\n","with open('/content/drive/MyDrive/pemrosesan text/hasil_scraping.csv', 'w', newline='') as file:\n","    writer = csv.writer(file)\n","    # Menulis header\n","    writer.writerow([\"No\", \"Judul Artikel\", \"Nama Jurnal\", \"Prosiding\", \"Penulis\", \"Tahun Terbit\", \"URL Artikel\", \"URL PDF\", \"Abstrak\"])\n","\n","    # Inisialisasi nomor baris\n","    row_number = 0\n","\n","    # Looping halaman\n","    for page in range(1, max_rows):\n","        # Jika jumlah baris maksimum telah tercapai, hentikan loop\n","        if row_number >= max_rows:\n","            break\n","\n","        # URL SpringerOpen\n","        url = f\"https://www.springeropen.com/search?searchType=publisherSearch&sort={sort}&query={kata_kunci}&page={page}\"\n","\n","        # Membuat permintaan ke SpringerOpen\n","        response = requests.get(url)\n","        soup = BeautifulSoup(response.text, 'html.parser')\n","\n","        # Mencari semua elemen dengan class 'c-list-group__item'\n","        results = soup.find_all('li', {'class': 'c-listing__item u-keyline'})\n","\n","        # Looping melalui setiap hasil dan mengekstrak informasi\n","        for result in results:\n","            # Tambahkan nomor baris\n","            row_number += 1\n","\n","            # Jika jumlah baris maksimum telah tercapai, hentikan loop\n","            if row_number > max_rows:\n","                break\n","\n","            title = result.find('h3', {'class': 'c-listing__title'}).text if result.find('h3', {'class': 'c-listing__title'}) else \"N/A\"\n","            journal_info = result.find('em',{'data-test': 'journal-title'}).text if result.find('em',{'data-test': 'journal-title'}) else \"N/A\"\n","            url_article_element = result.find('a')\n","            url_article = \"https:\" + url_article_element['href'] if url_article_element and url_article_element.has_attr('href') else \"N/A\"\n","\n","            author = result.find('span', {'class': 'c-listing__authors-list'}).text if result.find('span', {'class': 'c-listing__authors-list'}) else \"N/A\"\n","            year_published = result.find('span', {'itemprop': 'datePublished'}).text if result.find('span', {'itemprop': 'datePublished'}) else \"N/A\"\n","\n","            pdf_link_element = result.find('a', {'data-test': 'pdf-link'})\n","            pdf_link = \"https:\" + pdf_link_element['href'] if pdf_link_element and pdf_link_element.has_attr('href') else \"N/A\"\n","\n","            # Membuat permintaan ke halaman artikel\n","            url_abstract_element = result.find('a', itemprop='url')\n","            abstract_link = \"https:\" + url_abstract_element['href']\n","            article_response = requests.get(abstract_link)\n","            article_soup = BeautifulSoup(article_response.text, 'html.parser')\n","\n","            # Temukan div dengan class 'c-article-section__content'\n","            result_abstract = article_soup.find('div', {'class': 'c-article-section__content'})\n","\n","            # Ekstrak teks dalam tag <p> di dalam div\n","            abstract = result_abstract.find('p').text if result_abstract and result_abstract.find('p') else \"N/A\"\n","\n","            # Mengubah abstrak menjadi teks biasa (ASCII)\n","            abstract_plain = abstract.encode('ascii', 'ignore').decode('ascii')\n","\n","            # Menghapus spasi ekstra\n","            abstract_plain = \" \".join(abstract_plain.split())\n","\n","            # Menulis baris ke file CSV\n","            writer.writerow([row_number, title, journal_info, \"N/A\", author, year_published, url_article, pdf_link, abstract_plain])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import requests\n","from bs4 import BeautifulSoup\n","import time\n","import random\n","import re\n","import json\n","\n","# URL GSM Arena\n","url = \"https://www.gsmarena.com/makers.php3\"\n","\n","# Define a function to convert currency to IDR\n","def convert_to_idr(amount, from_currency):\n","    api_url = f\"https://api.exchangerate-api.com/v4/latest/{from_currency}\"\n","    response = requests.get(api_url)\n","    data = json.loads(response.text)\n","    return amount * data['rates']['IDR']\n","\n","\n","# Membuat permintaan ke GSM Arena\n","response = requests.get(url)\n","soup = BeautifulSoup(response.text, 'html.parser')\n","\n","# Mencari semua elemen dengan class 'st-text'\n","results = soup.find_all('div', {'class': 'st-text'})\n","\n","# Mengambil href dari setiap tag a dalam hasil pencarian\n","base_url = \"https://www.gsmarena.com/\"\n","for result in results:\n","    links = result.find_all('a', href=True)\n","    for link in links:\n","        time.sleep(random.uniform(1, 3))  # Add random delay between 1 to 3 seconds\n","        href = link['href']\n","        parts = href.split('-')\n","        maker, model, id = parts[0], parts[1], parts[2].split('.')[0]\n","        page_number = 1  # Start from page 1\n","        while True:  # Keep looping until there's no more pages\n","            new_href = f\"{maker}-{model}-f-{id}-0-p{page_number}.php\"\n","            href_merek = base_url + new_href\n","            merek_response = requests.get(href_merek)\n","            merek_soup = BeautifulSoup(merek_response.text, 'html.parser')\n","            link_merek = merek_soup.find_all('div', {'class': 'makers'}) \n","            if not link_merek:  # If there's no more pages, break the loop\n","                break\n","            for result_merek in link_merek:\n","                links_tipe = result_merek.find_all('a', href=True)\n","                for link_tipe in links_tipe:\n","                    time.sleep(random.randint(1, 3))  # Add delay between 1 to 3 seconds\n","                    href_tipe = base_url + link_tipe['href']\n","                    tipe_response = requests.get(href_tipe)\n","                    tipe_soup = BeautifulSoup(tipe_response.text, 'html.parser')\n","                    #Extract the release year\n","                    release_year_tag = tipe_soup.find('td', {'data-spec': 'year'})\n","                    if release_year_tag:\n","                        release_year = int(release_year_tag.text.split(', ')[1])  # Assuming the text is in the format 'Announced, 2020'\n","                        if release_year < 2020:\n","                            continue  # Skip this phone if it was released before 2020\n","                        # Extract the price\n","                        price_tag = tipe_soup.find('td', {'data-spec': 'price'}).text or tipe_soup.find_all('a', href=True).text\n","                        if len(price_tag) <20:\n","                            href_harga = links_tipe['href']\n","                            parts = href.split('-')\n","                            id_href_harga = parts[2].split('.')[0]\n","                            new_format = f\"price.php3?idPhone={id_href_harga.group}&sSaveCurrency=IDR\"\n","                            href_harga = base_url + new_format\n","                            harga_response = requests.get(href_harga)\n","                            harga_soup = BeautifulSoup(harga_response.text, 'html.parser')\n","                            harga_tag = harga_soup.find('a', {'data-url': re.compile(r'^price-js\\.php3')})\n","                            print(harga_tag)\n","                        else:\n","                            # Extract the numeric part of the price and the currency code\n","                            price_parts = price_tag.split(' ')\n","                            price_amount = float(price_parts[1].replace(',', ''))\n","                            price_currency = price_parts[2]\n","                            # Convert the price to IDR\n","                            price_in_idr = convert_to_idr(price_amount, price_currency)\n","                            print(price_in_idr)\n","            page_number += 1  # Increment the page number for the next iteration\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"1hKMJfmp-uX0aVowtr0-JjMlsZOMTguG3","timestamp":1696763377246}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
