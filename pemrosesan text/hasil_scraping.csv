No,Judul Artikel,Nama Jurnal,Prosiding,Penulis,Tahun Terbit,URL Artikel,URL PDF,Abstrak
1,"
Text-to-speech system for low-resource language using cross-lingual transfer learning and data augmentation
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Zolzaya Byambadorj, Ryota Nishimura, Altangerel Ayush, Kengo Ohta and Norihide Kitaoka",4 December 2021,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-021-00225-4,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-021-00225-4.pdf,"Deep learning techniques are currently being applied in automated text-to-speech (TTS) systems, resulting in significant improvements in performance. However, these methods require large amounts of text-speech paired data for model training, and collecting this data is costly. Therefore, in this paper, we propose a single-speaker TTS system containing both a spectrogram prediction network and a neural vocoder for the target language, using only 30 min of target language text-speech paired data for training. We evaluate three approaches for training the spectrogram prediction models of our TTS system, which produce mel-spectrograms from the input phoneme sequence: (1) cross-lingual transfer learning, (2) data augmentation, and (3) a combination of the previous two methods. In the cross-lingual transfer learning method, we used two high-resource language datasets, English (24 h) and Japanese (10 h). We also used 30 min of target language data for training in all three approaches, and for generating the augmented data used for training in methods 2 and 3. We found that using both cross-lingual transfer learning and augmented data during training resulted in the most natural synthesized target speech output. We also compare single-speaker and multi-speaker training methods, using sequential and simultaneous training, respectively. The multi-speaker models were found to be more effective for constructing a single-speaker, low-resource TTS model. In addition, we trained two Parallel WaveGAN (PWG) neural vocoders, one using 13 h of our augmented data with 30 min of target language data and one using the entire 12 h of the original target language dataset. Our subjective AB preference test indicated that the neural vocoder trained with augmented data achieved almost the same perceived speech quality as the vocoder trained with the entire target language dataset. Overall, we found that our proposed TTS system consisting of a spectrogram prediction network and a PWG neural vocoder was able to achieve reasonable performance using only 30 min of target language training data. We also found that by using 3 h of target language data, for training the model and for generating augmented data, our proposed TTS model was able to achieve performance very similar to that of the baseline model, which was trained with 12 h of target language data."
2,"
Depression-level assessment from multi-lingual conversational speech data using acoustic and text features
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Cenk Demiroglu, Aslı Beşirli, Yasin Ozkanca and Selime Çelik",17 November 2020,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-020-00182-4,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-020-00182-4.pdf,"Depression is a widespread mental health problem around the world with a significant burden on economies. Its early diagnosis and treatment are critical to reduce the costs and even save lives. One key aspect to achieve that goal is to use technology and monitor depression remotely and relatively inexpensively using automated agents. There has been numerous efforts to automatically assess depression levels using audiovisual features as well as text-analysis of conversational speech transcriptions. However, difficulty in data collection and the limited amounts of data available for research present challenges that are hampering the success of the algorithms. One of the two novel contributions in this paper is to exploit databases from multiple languages for acoustic feature selection. Since a large number of features can be extracted from speech, given the small amounts of training data available, effective data selection is critical for success. Our proposed multi-lingual method was effective at selecting better features than the baseline algorithms, which significantly improved the depression assessment accuracy. The second contribution of the paper is to extract text-based features for depression assessment and use a novel algorithm to fuse the text- and speech-based classifiers which further boosted the performance."
3,"
A unit selection text-to-speech-and-singing synthesis framework from neutral speech: proof of concept
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Marc Freixes, Francesc Alías and Joan Claudi Socoró",16 December 2019,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-019-0163-y,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-019-0163-y.pdf,"Text-to-speech (TTS) synthesis systems have been widely used in general-purpose applications based on the generation of speech. Nonetheless, there are some domains, such as storytelling or voice output aid devices, which may also require singing. To enable a corpus-based TTS system to sing, a supplementary singing database should be recorded. This solution, however, might be too costly for eventual singing needs, or even unfeasible if the original speaker is unavailable or unable to sing properly. This work introduces a unit selection-based text-to-speech-and-singing (US-TTS&S) synthesis framework, which integrates speech-to-singing (STS) conversion to enable the generation of both speech and singing from an input text and a score, respectively, using the same neutral speech corpus. The viability of the proposal is evaluated considering three vocal ranges and two tempos on a proof-of-concept implementation using a 2.6-h Spanish neutral speech corpus. The experiments show that challenging STS transformation factors are required to sing beyond the corpus vocal range and/or with notes longer than 150 ms. While score-driven US configurations allow the reduction of pitch-scale factors, time-scale factors are not reduced due to the short length of the spoken vowels. Moreover, in the MUSHRA test, text-driven and score-driven US configurations obtain similar naturalness rates of around 40 for all the analysed scenarios. Although these naturalness scores are far from those of vocaloid, the singing scores of around 60 which were obtained validate that the framework could reasonably address eventual singing needs."
4,"
Components loss for neural networks in mask-based speech enhancement
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Ziyi Xu, Samy Elshamy, Ziyue Zhao and Tim Fingscheidt",2 July 2021,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-021-00207-6,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-021-00207-6.pdf,"Estimating time-frequency domain masks for single-channel speech enhancement using deep learning methods has recently become a popular research field with promising results. In this paper, we propose a novel components loss (CL) for the training of neural networks for mask-based speech enhancement. During the training process, the proposed CL offers separate control over preservation of the speech component quality, suppression of the noise component, and preservation of a naturally sounding residual noise component. We illustrate the potential of the proposed CL by evaluating a standard convolutional neural network (CNN) for mask-based speech enhancement. The new CL is compared to several baseline losses, comprising the conventional mean squared error (MSE) loss w.r.t. speech spectral amplitudes or w.r.t. an ideal-ratio mask, auditory-related loss functions, such as the perceptual evaluation of speech quality (PESQ) loss and the perceptual weighting filter loss, and also the recently proposed SNR loss with two masks. Detailed analysis suggests that the proposed CL obtains a better or at least a more balanced performance across all employed instrumental quality metrics, including SNR improvement, speech component quality, enhanced total speech quality, and particularly also delivers a natural sounding residual noise component. For unseen noise types, we excel even perceptually motivated losses by an about 0.2 points higher PESQ score. The recently proposed so-called SNR loss with two masks not only requires a network with more parameters due to the two decoder heads, but also falls behind on PESQ and POLQA and particularly w.r.t. residual noise quality. Note that the proposed CL shows significantly more 1st ranks among the evaluation metrics than any other baseline. It is easy to implement, and code is provided at https://github.com/ifnspaml/Components-Loss."
5,"
Synthetic speech detection through short-term and long-term prediction traces
",EURASIP Journal on Information Security,N/A,"Clara Borrelli, Paolo Bestagini, Fabio Antonacci, Augusto Sarti and Stefano Tubaro",6 April 2021,https://jis-eurasipjournals.springeropen.com/articles/10.1186/s13635-021-00116-3,https://jis-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13635-021-00116-3.pdf,"Several methods for synthetic audio speech generation have been developed in the literature through the years. With the great technological advances brought by deep learning, many novel synthetic speech techniques achieving incredible realistic results have been recently proposed. As these methods generate convincing fake human voices, they can be used in a malicious way to negatively impact on today’s society (e.g., people impersonation, fake news spreading, opinion formation). For this reason, the ability of detecting whether a speech recording is synthetic or pristine is becoming an urgent necessity. In this work, we develop a synthetic speech detector. This takes as input an audio recording, extracts a series of hand-crafted features motivated by the speech-processing literature, and classify them in either closed-set or open-set. The proposed detector is validated on a publicly available dataset consisting of 17 synthetic speech generation algorithms ranging from old fashioned vocoders to modern deep learning solutions. Results show that the proposed method outperforms recently proposed detectors in the forensics literature."
6,"
An integrated MVDR beamformer for speech enhancement using a local microphone array and external microphones
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Randall Ali, Toon van Waterschoot and Marc Moonen",10 February 2021,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-020-00192-2,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-020-00192-2.pdf,"An integrated version of the minimum variance distortionless response (MVDR) beamformer for speech enhancement using a microphone array has been recently developed, which merges the benefits of imposing constraints defined from both a relative transfer function (RTF) vector based on a priori knowledge and an RTF vector based on a data-dependent estimate. In this paper, the integrated MVDR beamformer is extended for use with a microphone configuration where a microphone array, local to a speech processing device, has access to the signals from multiple external microphones (XMs) randomly located in the acoustic environment. The integrated MVDR beamformer is reformulated as a quadratically constrained quadratic program (QCQP) with two constraints, one of which is related to the maximum tolerable speech distortion for the imposition of the a priori RTF vector and the other related to the maximum tolerable speech distortion for the imposition of the data-dependent RTF vector. An analysis of how these maximum tolerable speech distortions affect the behaviour of the QCQP is presented, followed by the discussion of a general tuning framework. The integrated MVDR beamformer is then evaluated with audio recordings from behind-the-ear hearing aid microphones and three XMs for a single desired speech source in a noisy environment. In comparison to relying solely on an a priori RTF vector or a data-dependent RTF vector, the results demonstrate that the integrated MVDR beamformer can be tuned to yield different enhanced speech signals, which may be more suitable for improving speech intelligibility despite changes in the desired speech source position and imperfectly estimated spatial correlation matrices."
7,"
Detection of fake news and hate speech for Ethiopian languages: a systematic review of the approaches
",Journal of Big Data,N/A,Wubetu Barud Demilie and Ayodeji Olalekan Salau,19 May 2022,https://journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00619-x,https://journalofbigdata.springeropen.com/counter/pdf/10.1186/s40537-022-00619-x.pdf,"With the proliferation of social media platforms that provide anonymity, easy access, online community development, and online debate, detecting and tracking hate speech has become a major concern for society, individuals, policymakers, and researchers. Combating hate speech and fake news are the most pressing societal issues. It is difficult to expose false claims before they cause significant harm. Automatic fact or claim verification has recently piqued the interest of various research communities. Despite efforts to use automatic approaches for detection and monitoring, their results are still unsatisfactory, and that requires more research work in the area. Fake news and hate speech messages are any messages on social media platforms that spread negativity in society about sex, caste, religion, politics, race, disability, sexual orientation, and so on. Thus, the type of massage is extremely difficult to detect and combat. This work aims to analyze the optimal approaches for this kind of problem, as well as the relationship between the approaches, dataset type, size, and accuracy. Finally, based on the analysis results of the implemented approaches, deep learning (DL) approaches have been recommended for other Ethiopian languages to increase the performance of all evaluation metrics from different social media platforms. Additionally, as the review results indicate, the combination of DL and machine learning (ML) approaches with a balanced dataset can improve the detection and combating performance of the system."
8,"
A Tutorial on Text-Independent Speaker Verification
",EURASIP Journal on Advances in Signal Processing,N/A,"Frédéric Bimbot, Jean-François Bonastre, Corinne Fredouille, Guillaume Gravier, Ivan Magrin-Chagnolleau, Sylvain Meignier, Teva Merlin, Javier Ortega-García, Dijana Petrovska-Delacrétaz and Douglas A. Reynolds",21 April 2004,https://asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865704310024,https://asp-eurasipjournals.springeropen.com/counter/pdf/10.1155/S1110865704310024.pdf,"This paper presents an overview of a state-of-the-art text-independent speaker verification system. First, an introduction proposes a modular scheme of the training and test phases of a speaker verification system. Then, the most commonly speech parameterization used in speaker verification, namely, cepstral analysis, is detailed. Gaussian mixture modeling, which is the speaker modeling technique used in most systems, is then explained. A few speaker modeling alternatives, namely, neural networks and support vector machines, are mentioned. Normalization of scores is then explained, as this is a very important step to deal with real-world data. The evaluation of a speaker verification system is then detailed, and the detection error trade-off (DET) curve is explained. Several extensions of speaker verification are then enumerated, including speaker tracking and segmentation by speakers. Then, some applications of speaker verification are proposed, including on-site applications, remote applications, applications relative to structuring audio information, and games. Issues concerning the forensic area are then recalled, as we believe it is very important to inform people about the actual performance and limitations of speaker verification systems. This paper concludes by giving a few research trends in speaker verification for the next couple of years."
9,"
Efficient structured reporting in radiology using an intelligent dialogue system based on speech recognition and natural language processing
",Insights into Imaging,N/A,"Tobias Jorg, Benedikt Kämpgen, Dennis Feiler, Lukas Müller, Christoph Düber, Peter Mildenberger and Florian Jungmann",16 March 2023,https://insightsimaging.springeropen.com/articles/10.1186/s13244-023-01392-y,https://insightsimaging.springeropen.com/counter/pdf/10.1186/s13244-023-01392-y.pdf,"Structured reporting (SR) is recommended in radiology, due to its advantages over free-text reporting (FTR). However, SR use is hindered by insufficient integration of speech recognition, which is well accepted among radiologists and commonly used for unstructured FTR. SR templates must be laboriously completed using a mouse and keyboard, which may explain why SR use remains limited in clinical routine, despite its advantages. Artificial intelligence and related fields, like natural language processing (NLP), offer enormous possibilities to facilitate the imaging workflow. Here, we aimed to use the potential of NLP to combine the advantages of SR and speech recognition."
10,"
Adversarial joint training with self-attention mechanism for robust end-to-end speech recognition
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Lujun Li, Yikai Kang, Yuchen Shi, Ludwig Kürzinger, Tobias Watzel and Gerhard Rigoll",5 July 2021,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-021-00215-6,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-021-00215-6.pdf,"Lately, the self-attention mechanism has marked a new milestone in the field of automatic speech recognition (ASR). Nevertheless, its performance is susceptible to environmental intrusions as the system predicts the next output symbol depending on the full input sequence and the previous predictions. A popular solution for this problem is adding an independent speech enhancement module as the front-end. Nonetheless, due to being trained separately from the ASR module, the independent enhancement front-end falls into the sub-optimum easily. Besides, the handcrafted loss function of the enhancement module tends to introduce unseen distortions, which even degrade the ASR performance. Inspired by the extensive applications of the generative adversarial networks (GANs) in speech enhancement and ASR tasks, we propose an adversarial joint training framework with the self-attention mechanism to boost the noise robustness of the ASR system. Generally, it consists of a self-attention speech enhancement GAN and a self-attention end-to-end ASR model. There are two advantages which are worth noting in this proposed framework. One is that it benefits from the advancement of both self-attention mechanism and GANs, while the other is that the discriminator of GAN plays the role of the global discriminant network in the stage of the adversarial joint training, which guides the enhancement front-end to capture more compatible structures for the subsequent ASR module and thereby offsets the limitation of the separate training and handcrafted loss functions. With the adversarial joint optimization, the proposed framework is expected to learn more robust representations suitable for the ASR task. We execute systematic experiments on the corpus AISHELL-1, and the experimental results show that on the artificial noisy test set, the proposed framework achieves the relative improvements of 66% compared to the ASR model trained by clean data solely, 35.1% compared to the speech enhancement and ASR scheme without joint training, and 5.3% compared to multi-condition training."
11,"
Automatic Speech Recognition Systems for the Evaluation of Voice and Speech Disorders in Head and Neck Cancer
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Andreas Maier, Tino Haderlein, Florian Stelzle, Elmar Nöth, Emeka Nkenke, Frank Rosanowski, Anne Schützenberger and Maria Schuster",19 August 2009,https://asmp-eurasipjournals.springeropen.com/articles/10.1155/2010/926951,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1155/2010/926951.pdf,"In patients suffering from head and neck cancer, speech intelligibility is often restricted. For assessment and outcome measurements, automatic speech recognition systems have previously been shown to be appropriate for objective and quick evaluation of intelligibility. In this study we investigate the applicability of the method to speech disorders caused by head and neck cancer. Intelligibility was quantified by speech recognition on recordings of a standard text read by 41 German laryngectomized patients with cancer of the larynx or hypopharynx and 49 German patients who had suffered from oral cancer. The speech recognition provides the percentage of correctly recognized words of a sequence, that is, the word recognition rate. Automatic evaluation was compared to perceptual ratings by a panel of experts and to an age-matched control group. Both patient groups showed significantly lower word recognition rates than the control group. Automatic speech recognition yielded word recognition rates which complied with experts' evaluation of intelligibility on a significant level. Automatic speech recognition serves as a good means with low effort to objectify and quantify the most important aspect of pathologic speech—the intelligibility. The system was successfully applied to voice and speech disorders."
12,"
Part of speech tagging: a systematic review of deep learning and machine learning approaches
",Journal of Big Data,N/A,Alebachew Chiche and Betselot Yitagesu,24 January 2022,https://journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00561-y,https://journalofbigdata.springeropen.com/counter/pdf/10.1186/s40537-022-00561-y.pdf,"Natural language processing (NLP) tools have sparked a great deal of interest due to rapid improvements in information and communications technologies. As a result, many different NLP tools are being produced. However, there are many challenges for developing efficient and effective NLP tools that accurately process natural languages. One such tool is part of speech (POS) tagging, which tags a particular sentence or words in a paragraph by looking at the context of the sentence/words inside the paragraph. Despite enormous efforts by researchers, POS tagging still faces challenges in improving accuracy while reducing false-positive rates and in tagging unknown words. Furthermore, the presence of ambiguity when tagging terms with different contextual meanings inside a sentence cannot be overlooked. Recently, Deep learning (DL) and Machine learning (ML)-based POS taggers are being implemented as potential solutions to efficiently identify words in a given sentence across a paragraph. This article first clarifies the concept of part of speech POS tagging. It then provides the broad categorization based on the famous ML and DL techniques employed in designing and implementing part of speech taggers. A comprehensive review of the latest POS tagging articles is provided by discussing the weakness and strengths of the proposed approaches. Then, recent trends and advancements of DL and ML-based part-of-speech-taggers are presented in terms of the proposed approaches deployed and their performance evaluation metrics. Using the limitations of the proposed approaches, we emphasized various research gaps and presented future recommendations for the research in advancing DL and ML-based POS tagging."
13,"
An MMSE graph spectral magnitude estimator for speech signals residing on an undirected multiple graph
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Tingting Wang, Haiyan Guo, Zirui Ge, Qiquan Zhang and Zhen Yang",3 February 2023,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-023-00272-z,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-023-00272-z.pdf,"The paper uses the K-graphs learning method to construct weighted, connected, undirected multiple graphs, aiming to reveal intrinsic relationships of speech samples in the inter-frame and intra-frame. To benefit from the learned multiple graphs’ property and enhance interpretability, we study the spectral property of speech samples in the joint vertex-frequency domain by using the new graph weight matrix. Moreover, we propose the representation of minimum mean-square error (MMSE) graph spectral magnitude estimator for speech signals residing on undirected multiple graphs. We use the MMSE graph spectral magnitude estimator to improve speech enhancement performance. The numerical simulation results show that the proposed method outperforms the existing methods in graph signal processing (GSP) and the baseline methods for speech enhancement in discrete signal processing (DSP) in terms of PESQ, LLR, output SNR, and STOI results. These results also demonstrate the validity of the learned multiple graphs."
14,"
Room-localized speech activity detection in multi-microphone smart homes
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Panagiotis Giannoulis, Gerasimos Potamianos and Petros Maragos",27 August 2019,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-019-0158-8,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-019-0158-8.pdf,"Voice-enabled interaction systems in domestic environments have attracted significant interest recently, being the focus of smart home research projects and commercial voice assistant home devices. Within the multi-module pipelines of such systems, speech activity detection (SAD) constitutes a crucial component, providing input to their activation and speech recognition subsystems. In typical multi-room domestic environments, SAD may also convey spatial intelligence to the interaction, in addition to its traditional temporal segmentation output, by assigning speech activity at the room level. Such room-localized SAD can, for example, disambiguate user command referents, allow localized system feedback, and enable parallel voice interaction sessions by multiple subjects in different rooms. In this paper, we investigate a room-localized SAD system for smart homes equipped with multiple microphones distributed in multiple rooms, significantly extending our earlier work. The system employs a two-stage algorithm, incorporating a set of hand-crafted features specially designed to discriminate room-inside vs. room-outside speech at its second stage, refining SAD hypotheses obtained at its first stage by traditional statistical modeling and acoustic front-end processing. Both algorithmic stages exploit multi-microphone information, combining it at the signal, feature, or decision level. The proposed approach is extensively evaluated on both simulated and real data recorded in a multi-room, multi-microphone smart home, significantly outperforming alternative baselines. Further, it remains robust to reduced microphone setups, while also comparing favorably to deep learning-based alternatives."
15,"
On the Importance of Audiovisual Coherence for the Perceived Quality of Synthesized Visual Speech
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Wesley Mattheyses, Lukas Latacz and Werner Verhelst",22 September 2009,https://asmp-eurasipjournals.springeropen.com/articles/10.1155/2009/169819,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1155/2009/169819.pdf,"Audiovisual text-to-speech systems convert a written text into an audiovisual speech signal. Typically, the visual mode of the synthetic speech is synthesized separately from the audio, the latter being either natural or synthesized speech. However, the perception of mismatches between these two information streams requires experimental exploration since it could degrade the quality of the output. In order to increase the intermodal coherence in synthetic 2D photorealistic speech, we extended the well-known unit selection audio synthesis technique to work with multimodal segments containing original combinations of audio and video. Subjective experiments confirm that the audiovisual signals created by our multimodal synthesis strategy are indeed perceived as being more synchronous than those of systems in which both modes are not intrinsically coherent. Furthermore, it is shown that the degree of coherence between the auditory mode and the visual mode has an influence on the perceived quality of the synthetic visual speech fragment. In addition, the audio quality was found to have only a minor influence on the perceived visual signal's quality."
16,"
Dual supervised learning for non-native speech recognition
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Kacper Radzikowski, Robert Nowak, Le Wang and Osamu Yoshie",14 January 2019,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-018-0146-4,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-018-0146-4.pdf,"Current automatic speech recognition (ASR) systems achieve over 90–95% accuracy, depending on the methodology applied and datasets used. However, the level of accuracy decreases significantly when the same ASR system is used by a non-native speaker of the language to be recognized. At the same time, the volume of labeled datasets of non-native speech samples is extremely limited both in size and in the number of existing languages. This problem makes it difficult to train or build sufficiently accurate ASR systems targeted at non-native speakers, which, consequently, calls for a different approach that would make use of vast amounts of large unlabeled datasets. In this paper, we address this issue by employing dual supervised learning (DSL) and reinforcement learning with policy gradient methodology. We tested DSL in a warm-start approach, with two models trained beforehand, and in a semi warm-start approach with only one of the two models pre-trained. The experiments were conducted on English language pronounced by Japanese and Polish speakers. The results of our experiments show that creating ASR systems with DSL can achieve an accuracy comparable to traditional methods, while simultaneously making use of unlabeled data, which obviously is much cheaper to obtain and comes in larger sizes."
17,"
Search on speech from spoken queries: the Multi-domain International ALBAYZIN 2018 Query-by-Example Spoken Term Detection Evaluation
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Javier Tejedor, Doroteo T. Toledano, Paula Lopez-Otero, Laura Docio-Fernandez, Mikel Peñagarikano, Luis Javier Rodriguez-Fuentes and Antonio Moreno-Sandoval",19 July 2019,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-019-0156-x,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-019-0156-x.pdf,"The huge amount of information stored in audio and video repositories makes search on speech (SoS) a priority area nowadays. Within SoS, Query-by-Example Spoken Term Detection (QbE STD) aims to retrieve data from a speech repository given a spoken query. Research on this area is continuously fostered with the organization of QbE STD evaluations. This paper presents a multi-domain internationally open evaluation for QbE STD in Spanish. The evaluation aims at retrieving the speech files that contain the queries, providing their start and end times, and a score that reflects the confidence given to the detection. Three different Spanish speech databases that encompass different domains have been employed in the evaluation: MAVIR database, which comprises a set of talks from workshops; RTVE database, which includes broadcast television (TV) shows; and COREMAH database, which contains 2-people spontaneous speech conversations about different topics. The evaluation has been designed carefully so that several analyses of the main results can be carried out. We present the evaluation itself, the three databases, the evaluation metrics, the systems submitted to the evaluation, the results, and the detailed post-evaluation analyses based on some query properties (within-vocabulary/out-of-vocabulary queries, single-word/multi-word queries, and native/foreign queries). Fusion results of the primary systems submitted to the evaluation are also presented. Three different teams took part in the evaluation, and ten different systems were submitted. The results suggest that the QbE STD task is still in progress, and the performance of these systems is highly sensitive to changes in the data domain. Nevertheless, QbE STD strategies are able to outperform text-based STD in unseen data domains."
18,"
Lightweight multi-DOA tracking of mobile speech sources
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Caleb Rascon, Gibran Fuentes and Ivan Meza",7 May 2015,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-015-0055-8,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-015-0055-8.pdf,"Estimating the directions of arrival (DOAs) of multiple simultaneous mobile sound sources is an important step for various audio signal processing applications. In this contribution, we present an approach that improves upon our previous work that is now able to estimate the DOAs of multiple mobile speech sources, while being light in resources, both hardware-wise (only using three microphones) and software-wise. This approach takes advantage of the fact that simultaneous speech sources do not completely overlap each other. To evaluate the performance of this approach, a multi-DOA estimation evaluation system was developed based on a corpus collected from different acoustic scenarios named Acoustic Interactions for Robot Audition (AIRA)."
19,"
Benefits of pre-trained mono- and cross-lingual speech representations for spoken language understanding of Dutch dysarthric speech
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,Pu Wang and Hugo Van hamme,7 April 2023,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-023-00280-z,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-023-00280-z.pdf,"With the rise of deep learning, spoken language understanding (SLU) for command-and-control applications such as a voice-controlled virtual assistant can offer reliable hands-free operation to physically disabled individuals. However, due to data scarcity, it is still a challenge to process dysarthric speech. Pre-training (part of) the SLU model with supervised automatic speech recognition (ASR) targets or with self-supervised learning (SSL) may help to overcome a lack of data, but no research has shown which pre-training strategy performs better for SLU on dysarthric speech and to which extent the SLU task benefits from knowledge transfer from pre-training with dysarthric acoustic tasks. This work aims to compare different mono- or cross-lingual pre-training (supervised and unsupervised) methodologies and quantitatively investigates the benefits of pre-training for SLU tasks on Dutch dysarthric speech. The designed SLU systems consist of a pre-trained speech representations encoder and a SLU decoder to map encoded features to intents. Four types of pre-trained encoders, a mono-lingual time-delay neural network (TDNN) acoustic model, a mono-lingual transformer acoustic model, a cross-lingual transformer acoustic model (Whisper), and a cross-lingual SSL Wav2Vec2.0 model (XLSR-53), are evaluated complemented with three types of SLU decoders: non-negative matrix factorization (NMF), capsule networks, and long short-term memory (LSTM) networks. The acoustic analysis of the four pre-trained encoders are tested on Dutch dysarthric home-automation data with word error rate (WER) results to investigate the correlations of the dysarthric acoustic task (ASR) and the semantic task (SLU). By introducing the intelligibility score (IS) as a metric of the impairment severity, this paper further quantitatively analyzes dysarthria-severity-dependent models for SLU tasks."
20,"
Linking speech enhancement and error concealment based on recursive MMSE estimation
",EURASIP Journal on Advances in Signal Processing,N/A,"Balázs Fodor, Florian Pflug and Tim Fingscheidt",20 February 2015,https://asp-eurasipjournals.springeropen.com/articles/10.1186/s13634-015-0201-6,https://asp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13634-015-0201-6.pdf,"Speech enhancement and error concealment have seen a considerable progress over the past decades. Although both fields deal with distorted speech signals, there has rarely been an attempt to relate respective approaches to each other. In this paper, for the first time, a clear synopsis of recursive minimum mean square error (MMSE) estimation in both fields is provided. Our work intentionally does not propose a certain algorithm furthering the state of the art, nor does it provide simulation results of algorithms. Instead, our aim is threefold: First we revisit the basics of Bayes estimation in a recursive manner, covering both kinds of distortion acoustic noise as well as transmission channel noise. Second, we present recursive MMSE estimation applied to speech enhancement (in the frequency domain, as typical) and applied to error concealment (in the time domain, as typical) in strictly coherent notations and provide respective overview diagrams. Finally, we discuss commonalities and differences between both approaches, identify a particular strength of error concealment in general, and provide possible research directions for speech enhancement. A particularly interesting observation is that noise introduced by error concealment is far from being Gaussian and that additive acoustic noise can be expressed in terms of bit errors in DFT coefficients providing a potential interface to error concealment approaches."
21,"
Speech enhancement by LSTM-based noise suppression followed by CNN-based speech restoration
",EURASIP Journal on Advances in Signal Processing,N/A,"Maximilian Strake, Bruno Defraene, Kristoff Fluyt, Wouter Tirry and Tim Fingscheidt",10 December 2020,https://asp-eurasipjournals.springeropen.com/articles/10.1186/s13634-020-00707-1,https://asp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13634-020-00707-1.pdf,"Single-channel speech enhancement in highly non-stationary noise conditions is a very challenging task, especially when interfering speech is included in the noise. Deep learning-based approaches have notably improved the performance of speech enhancement algorithms under such conditions, but still introduce speech distortions if strong noise suppression shall be achieved. We propose to address this problem by using a two-stage approach, first performing noise suppression and subsequently restoring natural sounding speech, using specifically chosen neural network topologies and loss functions for each task. A mask-based long short-term memory (LSTM) network is employed for noise suppression and speech restoration is performed via spectral mapping with a convolutional encoder-decoder network (CED). The proposed method improves speech quality (PESQ) over state-of-the-art single-stage methods by about 0.1 points for unseen highly non-stationary noise types including interfering speech. Furthermore, it is able to increase intelligibility in low-SNR conditions and consistently outperforms all reference methods."
22,"
Speech enhancement from fused features based on deep neural network and gated recurrent unit network
",EURASIP Journal on Advances in Signal Processing,N/A,"Youming Wang, Jiali Han, Tianqi Zhang and Didi Qing",24 October 2021,https://asp-eurasipjournals.springeropen.com/articles/10.1186/s13634-021-00813-8,https://asp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13634-021-00813-8.pdf,"Speech is easily interfered by external environment in reality, which results in the loss of important features. Deep learning has become a popular speech enhancement method because of its superior potential in solving nonlinear mapping problems for complex features. However, the deficiency of traditional deep learning methods is the weak learning capability of important information from previous time steps and long-term event dependencies between the time-series data. To overcome this problem, we propose a novel speech enhancement method based on the fused features of deep neural networks (DNNs) and gated recurrent unit (GRU). The proposed method uses GRU to reduce the number of parameters of DNNs and acquire the context information of the speech, which improves the enhanced speech quality and intelligibility. Firstly, DNN with multiple hidden layers is used to learn the mapping relationship between the logarithmic power spectrum (LPS) features of noisy speech and clean speech. Secondly, the LPS feature of the deep neural network is fused with the noisy speech as the input of GRU network to compensate the missing context information. Finally, GRU network is performed to learn the mapping relationship between LPS features and log power spectrum features of clean speech spectrum. The proposed model is experimentally compared with traditional speech enhancement models, including DNN, CNN, LSTM and GRU. Experimental results demonstrate that the PESQ, SSNR and STOI of the proposed algorithm are improved by 30.72%, 39.84% and 5.53%, respectively, compared with the noise signal under the condition of matched noise. Under the condition of unmatched noise, the PESQ and STOI of the algorithm are improved by 23.8% and 37.36%, respectively. The advantage of the proposed method is that it uses the key information of features to suppress noise in both matched and unmatched noise cases and the proposed method outperforms other common methods in speech enhancement."
23,"
A large vocabulary continuous speech recognition system for Persian language
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Hossein Sameti, Hadi Veisi, Mohammad Bahrani, Bagher Babaali and Khosro Hosseinzadeh",5 October 2011,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/1687-4722-2011-426795,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/1687-4722-2011-426795.pdf,"The first large vocabulary speech recognition system for the Persian language is introduced in this paper. This continuous speech recognition system uses most standard and state-of-the-art speech and language modeling techniques. The development of the system, called Nevisa, has been started in 2003 with a dominant academic theme. This engine incorporates customized established components of traditional continuous speech recognizers and its parameters have been optimized for real applications of the Persian language. For this purpose, we had to identify the computational challenges of the Persian language, especially for text processing and extract statistical and grammatical language models for the Persian language. To achieve this, we had to either generate the necessary speech and text corpora or modify the available primitive corpora available for the Persian language."
24,"
Performance vs. hardware requirements in state-of-the-art automatic speech recognition
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Alexandru-Lucian Georgescu, Alessandro Pappalardo, Horia Cucu and Michaela Blott",21 July 2021,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-021-00217-4,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-021-00217-4.pdf,"The last decade brought significant advances in automatic speech recognition (ASR) thanks to the evolution of deep learning methods. ASR systems evolved from pipeline-based systems, that modeled hand-crafted speech features with probabilistic frameworks and generated phone posteriors, to end-to-end (E2E) systems, that translate the raw waveform directly into words using one deep neural network (DNN). The transcription accuracy greatly increased, leading to ASR technology being integrated into many commercial applications. However, few of the existing ASR technologies are suitable for integration in embedded applications, due to their hard constrains related to computing power and memory usage. This overview paper serves as a guided tour through the recent literature on speech recognition and compares the most popular ASR implementations. The comparison emphasizes the trade-off between ASR performance and hardware requirements, to further serve decision makers in choosing the system which fits best their embedded application. To the best of our knowledge, this is the first study to provide this kind of trade-off analysis for state-of-the-art ASR systems."
25,"
Comparative analysis of deep learning based Afaan Oromo hate speech detection
",Journal of Big Data,N/A,Gaddisa Olani Ganfure,2 June 2022,https://journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00628-w,https://journalofbigdata.springeropen.com/counter/pdf/10.1186/s40537-022-00628-w.pdf,"Social media platforms like Facebook, YouTube, and Twitter are banking on developing machine learning models to help stop the spread of hateful speech on their platforms. The idea is that machine learning models that utilize natural language processing will detect hate speech faster and better than people can. Despite numerous progress has been made for resource reach language, only a few attempts have been made for Ethiopian Languages such as Afaan Oromo. This paper examines the viability of deep learning models for Afaan Oromo hate speech recognition. Toward this, the biggest dataset of hate speech was collected and annotated by the language experts. Variations of profound deep learning models such as CNN, LSTMs, BiLSTMs, LSTM, GRU, and CNN-LSTM are examined to evaluate their viability in identifying Afaan Oromo Hate speeches. The result uncovers that the model dependent on CNN and Bi-LSTM outperforms all the other investigated models with an average F1-score of 87%."
26,"
An adaptive a priori SNR estimator for perceptual speech enhancement
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Lara Nahma, Pei Chee Yong, Hai Huyen Dam and Sven Nordholm",7 June 2019,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-019-0150-3,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-019-0150-3.pdf,"In this paper, an adaptive averaging a priori SNR estimation employing critical band processing is proposed. The proposed method modifies the current decision-directed a priori SNR estimation to achieve faster tracking when SNR changes. The decision-directed estimator (DD) employs a fixed weighting with the value close to one, which makes it slow in following the onsets of speech utterances. The proposed SNR estimator provides a means to solve this issue by employing an adaptive weighting factor. This allows an improved tracking of onset changes in the speech signal. As a consequence, it results in better preservation of speech components. This adaptive technique ensures that the weighting between the modified decision-directed a priori estimate and the maximum likelihood a priori estimate is a function of the speech absence probability. The estimate of the speech absence probability is modeled by a sigmoid function. Furthermore, a critical band mapping for the short-time Fourier transform analysis-synthesis system is utilized in the speech enhancement to achieve less musical noise. In addition, to evaluate the ability of the a priori SNR estimation method in preserving speech components, we proposed a modified objective measurement known as modified hamming distance. Evaluations are performed by utilizing both objective and subjective measurements. The experimental results show that the proposed method improves the speech quality under different noise conditions. Moreover, it maintains the advantage of the DD approach in eliminating the musical noise under different SNR conditions. The objective results are supported by subjective listening tests using 10 subjects (5 males and 5 females)."
27,"
Exploiting Speech for Automatic TV Delinearization: From Streams to Cross-Media Semantic Navigation
",EURASIP Journal on Image and Video Processing,N/A,"Guillaume Gravier, Camille Guinaudeau, Gwénolé Lecorvé and Pascale Sébillot",7 February 2011,https://jivp-eurasipjournals.springeropen.com/articles/10.1155/2011/689780,https://jivp-eurasipjournals.springeropen.com/counter/pdf/10.1155/2011/689780.pdf,"The gradual migration of television from broadcast diffusion to Internet diffusion offers countless possibilities for the generation of rich navigable contents. However, it also raises numerous scientific issues regarding delinearization of TV streams and content enrichment. In this paper, we study how speech can be used at different levels of the delinearization process, using automatic speech transcription and natural language processing (NLP) for the segmentation and characterization of TV programs and for the generation of semantic hyperlinks in videos. Transcript-based video delinearization requires natural language processing techniques robust to transcription peculiarities, such as transcription errors, and to domain and genre differences. We therefore propose to modify classical NLP techniques, initially designed for regular texts, to improve their robustness in the context of TV delinearization. We demonstrate that the modified NLP techniques can efficiently handle various types of TV material and be exploited for program description, for topic segmentation, and for the generation of semantic hyperlinks between multimedia contents. We illustrate the concept of cross-media semantic navigation with a description of our news navigation demonstrator presented during the NEM Summit 2009."
28,"
Dynamic out-of-vocabulary word registration to language model for speech recognition
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Norihide Kitaoka, Bohan Chen and Yuya Obashi",25 January 2021,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-020-00193-1,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-020-00193-1.pdf,"We propose a method of dynamically registering out-of-vocabulary (OOV) words by assigning the pronunciations of these words to pre-inserted OOV tokens, editing the pronunciations of the tokens. To do this, we add OOV tokens to an additional, partial copy of our corpus, either randomly or to part-of-speech (POS) tags in the selected utterances, when training the language model (LM) for speech recognition. This results in an LM containing OOV tokens, to which we can assign pronunciations. We also investigate the impact of acoustic complexity and the “natural” occurrence frequency of OOV words on the recognition of registered OOV words. The proposed OOV word registration method is evaluated using two modern automatic speech recognition (ASR) systems, Julius and Kaldi, using DNN-HMM acoustic models and N-gram language models (plus an additional evaluation using RNN re-scoring with Kaldi). Our experimental results show that when using the proposed OOV registration method, modern ASR systems can recognize OOV words without re-training the language model, that the acoustic complexity of OOV words affects OOV recognition, and that differences between the “natural” and the assigned occurrence frequencies of OOV words have little impact on the final recognition results."
29,"
Review of methods for coding of speech signals
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,Douglas O’Shaughnessy,7 February 2023,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-023-00274-x,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-023-00274-x.pdf,"Speech is the most common form of human communication, and many conversations use digital communication links. For efficient transmission, acoustic speech waveforms are usually converted to digital form, with reduced bit rates, while maintaining decoded speech quality. This paper reviews the history of speech coding techniques, from early mu-law logarithmic compression to recent neural-network methods. The techniques are examined in terms of output quality, algorithmic complexity, delay, and cost. Focus is on which aspects of speech can be exploited for high-quality transmission. The choices made to code speech are motivated by efficiency, the needs of applications, and access to information in the speech signal that is useful for both intelligibility and naturalness in the reconstructed speech at the decoder."
30,"
Classification of heterogeneous text data for robust domain-specific language modeling
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Ján Staš, Jozef Juhár and Daniel Hládek",15 April 2014,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/1687-4722-2014-14,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/1687-4722-2014-14.pdf,"The robustness of n-gram language models depends on the quality of text data on which they have been trained. The text corpora collected from various resources such as web pages or electronic documents are characterized by many possible topics. In order to build efficient and robust domain-specific language models, it is necessary to separate domain-oriented segments from the large amount of text data, and the remaining out-of-domain data can be used only for updating of existing in-domain n-gram probability estimates. In this paper, we describe the process of classification of heterogeneous text data into two classes, to the in-domain and out-of-domain data, mainly used for language modeling in the task-oriented speech recognition from judicial domain. The proposed algorithm for text classification is based on detection of theme in short text segments based on the most frequent key phrases. In the next step, each text segment is represented in vector space model as a feature vector with term weighting. For classification of these text segments to the in-domain and out-of domain area, document similarity with automatic thresholding are used. The experimental results of modeling the Slovak language and adaptation to the judicial domain show significant improvement in the model perplexity and increasing the performance of the Slovak transcription and dictation system."
31,"
Accent modification for speech recognition of non-native speakers using neural style transfer
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Kacper Radzikowski, Le Wang, Osamu Yoshie and Robert Nowak",18 February 2021,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-021-00199-3,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-021-00199-3.pdf,"Nowadays automatic speech recognition (ASR) systems can achieve higher and higher accuracy rates depending on the methodology applied and datasets used. The rate decreases significantly when the ASR system is being used with a non-native speaker of the language to be recognized. The main reason for this is specific pronunciation and accent features related to the mother tongue of that speaker, which influence the pronunciation. At the same time, an extremely limited volume of labeled non-native speech datasets makes it difficult to train, from the ground up, sufficiently accurate ASR systems for non-native speakers.In this research, we address the problem and its influence on the accuracy of ASR systems, using the style transfer methodology. We designed a pipeline for modifying the speech of a non-native speaker so that it more closely resembles the native speech. This paper covers experiments for accent modification using different setups and different approaches, including neural style transfer and autoencoder. The experiments were conducted on English language pronounced by Japanese speakers (UME-ERJ dataset). The results show that there is a significant relative improvement in terms of the speech recognition accuracy. Our methodology reduces the necessity of training new algorithms for non-native speech (thus overcoming the obstacle related to the data scarcity) and can be used as a wrapper for any existing ASR system. The modification can be performed in real time, before a sample is passed into the speech recognition system itself."
32,"
A Statistical Approach to Automatic Speech Summarization
",EURASIP Journal on Advances in Signal Processing,N/A,"Chiori Hori, Sadaoki Furui, Rob Malkin, Hua Yu and Alex Waibel",25 February 2003,https://asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865703211112,https://asp-eurasipjournals.springeropen.com/counter/pdf/10.1155/S1110865703211112.pdf,"This paper proposes a statistical approach to automatic speech summarization. In our method, a set of words maximizing a summarization score indicating the appropriateness of summarization is extracted from automatically transcribed speech and then concatenated to create a summary. The extraction process is performed using a dynamic programming (DP) technique based on a target compression ratio. In this paper, we demonstrate how an English news broadcast transcribed by a speech recognizer is automatically summarized. We adapted our method, which was originally proposed for Japanese, to English by modifying the model for estimating word concatenation probabilities based on a dependency structure in the original speech given by a stochastic dependency context free grammar (SDCFG). We also propose a method of summarizing multiple utterances using a two-level DP technique. The automatically summarized sentences are evaluated by summarization accuracy based on a comparison with a manual summary of speech that has been correctly transcribed by human subjects. Our experimental results indicate that the method we propose can effectively extract relatively important information and remove redundant and irrelevant information from English news broadcasts."
33,"
Single-channel dereverberation by feature mapping using cascade neural networks for robust distant speaker identification and speech recognition
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Aditya Arie Nugraha, Kazumasa Yamamoto and Seiichi Nakagawa",10 April 2014,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/1687-4722-2014-13,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/1687-4722-2014-13.pdf,"We present a feature enhancement method that uses neural networks (NNs) to map the reverberant feature in a log-melspectral domain to its corresponding anechoic feature. The mapping is done by cascade NNs trained using Cascade2 algorithm with an implementation of segment-based normalization. Experiments using speaker identification (SID) and automatic speech recognition (ASR) systems were conducted to evaluate the method. The experiments of SID system was conducted by using our own simulated and real reverberant datasets, while the CENSREC-4 evaluation framework was used as the evaluation for the ASR system. The proposed method could remarkably improve the performance of both systems by using limited stereo data and low speaker-variant data as the training data. From the evaluation using SID, we reached 26.0% and 34.8% of error rate reduction (ERR) relative to the baseline by using simulated and real data, respectively, by using only one pair of utterances for matched condition cases. Then, by using combined dataset containing 15 pairs of utterances by one speaker from three positions in a room, we could reach 93.7% of average identification rate (three known and two unknown positions), which was 42.2% of ERR relative to the use of cepstral mean normalization (CMN). From the evaluation using ASR, by using 40 pairs of utterances as the NN training data, we could reach 78.4% of ERR relative to the baseline by using simulated utterances by five speakers. Moreover, we could reach 75.4% and 71.6% of ERR relative to the baseline by using real utterances by five speakers and one speaker, respectively."
34,"
Semantic Indexing of Multimedia Content Using Visual, Audio, and Text Cues
",EURASIP Journal on Advances in Signal Processing,N/A,"W. H. Adams, Giridharan Iyengar, Ching-Yung Lin, Milind Ramesh Naphade, Chalapathy Neti, Harriet J. Nock and John R. Smith",25 February 2003,https://asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865703211173,https://asp-eurasipjournals.springeropen.com/counter/pdf/10.1155/S1110865703211173.pdf,"We present a learning-based approach to the semantic indexing of multimedia content using cues derived from audio, visual, and text features. We approach the problem by developing a set of statistical models for a predefined lexicon. Novel concepts are then mapped in terms of the concepts in the lexicon. To achieve robust detection of concepts, we exploit features from multiple modalities, namely, audio, video, and text. Concept representations are modeled using Gaussian mixture models (GMM), hidden Markov models (HMM), and support vector machines (SVM). Models such as Bayesian networks and SVMs are used in a late-fusion approach to model concepts that are not explicitly modeled in terms of features. Our experiments indicate promise in the proposed classification and fusion methodologies: our proposed fusion scheme achieves more than 10% relative improvement over the best unimodal concept detector."
35,"
Advanced acoustic modelling techniques in MP3 speech recognition
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Michal Borsky, Petr Pollak and Petr Mizera",28 July 2015,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-015-0064-7,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-015-0064-7.pdf,"The automatic recognition of MP3 compressed speech presents a challenge to the current systems due to the lossy nature of compression which causes irreversible degradation of the speech wave. This article evaluates the performance of a recognition system optimized for MP3 compressed speech with current state-of-the-art acoustic modelling techniques and one specific front-end compensation method. The article concentrates on acoustic model adaptation, discriminative training, and additional dithering as prominent means of compensating for the described distortion in the task of phoneme and large vocabulary continuous speech recognition (LVCSR). The experiments presented on the phoneme task show a dramatic increase of the recognition error for unvoiced speech units as a direct result of compression. The application of acoustic model adaptation has proved to yield the highest relative contribution while the gain of discriminative training diminished with decreasing bit-rate. The application of additional dithering yielded a consistent improvement only for the MFCC features, but the overall results were still worse than those for the PLP features."
36,"
Deficient-basis-complementary rank-constrained spatial covariance matrix estimation based on multivariate generalized Gaussian distribution for blind speech extraction
",EURASIP Journal on Advances in Signal Processing,N/A,"Yuto Kondo, Yuki Kubo, Norihiro Takamune, Daichi Kitamura and Hiroshi Saruwatari",22 September 2022,https://asp-eurasipjournals.springeropen.com/articles/10.1186/s13634-022-00905-z,https://asp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13634-022-00905-z.pdf,"Rank-constrained spatial covariance matrix estimation (RCSCME) is a blind speech extraction method utilized under the condition that one-directional target speech and diffuse background noise are mixed. In this paper, we propose a new model extension of RCSCME. RCSCME simultaneously conducts both the deficient rank-1 component complementation of the diffuse noise spatial covariance matrix, which is incompletely estimated by preprocessing methods such as independent low-rank matrix analysis, and the estimation of the source model parameters. In the conventional RCSCME, between the two parameters constituting the deficient rank-1 component, only the scale is estimated, whereas the other parameter, the deficient basis, is fixed in advance; however, how to choose the fixed deficient basis is not unique. In the proposed RCSCME model, we also regard the deficient basis as a parameter to estimate. As the generative model of an observed signal, we utilized the super-Gaussian generalized Gaussian distribution, which achieves better separation performance than the Gaussian distribution in the conventional RCSCME. Assuming the model, we derive new majorization-minimization (MM)- and majorization-equalization (ME)-algorithm-based update rules for the deficient basis. In particular, among innumerable ME-algorithm-based update rules, we successfully find an ME-algorithm-based update rule with a mathematical proof supporting the fact that the step of the update rule is larger than that of the MM-algorithm-based update rule. We confirm that the proposed method outperforms conventional methods under several simulated noise conditions and a real noise condition."
37,"
Speech enhancement for multimicrophone binaural hearing aids aiming to preserve the spatial auditory scene
",EURASIP Journal on Advances in Signal Processing,N/A,"Joachim Thiemann, Menno Müller, Daniel Marquardt, Simon Doclo and Steven van de Par",2 February 2016,https://asp-eurasipjournals.springeropen.com/articles/10.1186/s13634-016-0314-6,https://asp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13634-016-0314-6.pdf,"Modern binaural hearing aids utilize multimicrophone speech enhancement algorithms to enhance signals in terms of signal-to-noise ratio, but they may distort the interaural cues that allow the user to localize sources, in particular, suppressed interfering sources or background noise. In this paper, we present a novel algorithm that enhances the target signal while aiming to maintain the correct spatial rendering of both the target signal as well as the background noise. We use a bimodal approach, where a signal-to-noise ratio (SNR) estimator controls a binary decision mask, switching between the output signals of a binaural minimum variance distortionless response (MVDR) beamformer and scaled reference microphone signals. We show that the proposed selective binaural beamformer (SBB) can enhance the target signal while maintaining the overall spatial rendering of the acoustic scene."
38,"
Combination of MVDR beamforming and single-channel spectral processing for enhancing noisy and reverberant speech
",EURASIP Journal on Advances in Signal Processing,N/A,"Benjamin Cauchi, Ina Kodrasi, Robert Rehr, Stephan Gerlach, Ante Jukić, Timo Gerkmann, Simon Doclo and Stefan Goetze",23 July 2015,https://asp-eurasipjournals.springeropen.com/articles/10.1186/s13634-015-0242-x,https://asp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13634-015-0242-x.pdf,"This paper presents a system aiming at joint dereverberation and noise reduction by applying a combination of a beamformer with a single-channel spectral enhancement scheme. First, a minimum variance distortionless response beamformer with an online estimated noise coherence matrix is used to suppress noise and reverberation. The output of this beamformer is then processed by a single-channel spectral enhancement scheme, based on statistical room acoustics, minimum statistics, and temporal cepstrum smoothing, to suppress residual noise and reverberation. The evaluation is conducted using the REVERB challenge corpus, designed to evaluate speech enhancement algorithms in the presence of both reverberation and noise. The proposed system is evaluated using instrumental speech quality measures, the performance of an automatic speech recognition system, and a subjective evaluation of the speech quality based on a MUSHRA test. The performance achieved by beamforming, single-channel spectral enhancement, and their combination are compared, and experimental results show that the proposed system is effective in suppressing both reverberation and noise while improving the speech quality. The achieved improvements are particularly significant in conditions with high reverberation times."
39,"
Line, please? An analysis of the rehearsed speech characteristics of native Korean speakers on the English Oral Proficiency Interview—Computer (OPIc)
",Language Testing in Asia,N/A,"Gwyneth Gates, Troy L. Cox, Teresa Reber Bell and William Eggington",12 November 2020,https://languagetestingasia.springeropen.com/articles/10.1186/s40468-020-00110-5,https://languagetestingasia.springeropen.com/counter/pdf/10.1186/s40468-020-00110-5.pdf,"Two assumptions of speaking proficiency tests are that the speech produced is spontaneous and the the scores on those tests predict what examinees can do in real-world communicative situations. Therefore, when examinees memorize scripts for their oral responses, the validity of the score interpretation is threatened. While the American Council on the Teaching of Foreign Languages (ACTFL) Proficiency Guidelines identify rehearsed content as a major hindrance to interviewees being rated above Novice High, many examinees still prepare for speaking tests by memorizing and rehearsing scripts hoping these ""performances"" are awarded higher scores. To investigate this phenomenon, researchers screened 300 previously rated Oral Proficiency Interview-computer (OPIc) tests and found 39 examinees who had at least one response that had been tagged as rehearsed. Each examinee’s responses were then transcribed, and the spontaneous and rehearsed tasks were compared. Temporal fluency articulation rates differed significantly between the spontaneous and rehearsed segments; however, the strongest evidence of memorization lay in the transcriptions and the patterns that emerged within and across interviews. Test developers, therefore, need to be vigilant in creating scoring guidelines for rehearsed content."
40,"
A signal subspace approach to spatio-temporal prediction for multichannel speech enhancement
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,Adam Borowicz,10 February 2015,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-015-0051-z,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-015-0051-z.pdf,"The spatio-temporal-prediction (STP) method for multichannel speech enhancement has recently been proposed. This approach makes it theoretically possible to attenuate the residual noise without distorting speech. In addition, the STP method depends only on the second-order statistics and can be implemented using a simple linear filtering framework. Unfortunately, some numerical problems can arise when estimating the filter matrix in transients. In such a case, the speech correlation matrix is usually rank deficient, so that no solution exists. In this paper, we propose to implement the spatio-temporal-prediction method using a signal subspace approach. This allows for nullifying the noise subspace and processing only the noisy signal in the signal-plus-noise subspace. As a result, we are able to not only regularize the solution in transients but also to achieve higher attenuation of the residual noise. The experimental results also show that the signal subspace approach distorts speech less than the conventional method."
41,"
Free tools and resources for Brazilian Portuguese speech recognition
",Journal of the Brazilian Computer Society,N/A,"Nelson Neto, Carlos Patrick, Aldebaro Klautau and Isabel Trancoso",4 November 2010,https://journal-bcs.springeropen.com/articles/10.1007/s13173-010-0023-1,https://journal-bcs.springeropen.com/counter/pdf/10.1007/s13173-010-0023-1.pdf,"An automatic speech recognition system has modules that depend on the language and, while there are many public resources for some languages (e.g., English and Japanese), the resources for Brazilian Portuguese (BP) are still limited. This work describes the development of resources and free tools for BP speech recognition, consisting of text and audio corpora, phonetic dictionary, grapheme-to-phone converter, language and acoustic models. All of them are publicly available and, together with a proposed application programming interface, have been used for the development of several new applications, including a speech module for the OpenOffice suite. Performance tests are presented, comparing the developed BP system with a commercial software. The paper also describes an application that uses synthesis and speech recognition together with a natural language processing module dedicated to statistical machine translation. This application allows the translation of spoken conversations from BP to English and vice versa. The resources make easier the adoption of BP speech technologies by other academic groups and industry."
42,"
A comparative study of the effect of teacher’s group and individual feedback on Iranian EFL learners’ learning of speech acts in apology letter writing
",Asian-Pacific Journal of Second and Foreign Language Education,N/A,Zahra Pourmousavi and Zohre Mohamadi Zenouzagh,20 August 2020,https://sfleducation.springeropen.com/articles/10.1186/s40862-020-00088-w,https://sfleducation.springeropen.com/counter/pdf/10.1186/s40862-020-00088-w.pdf,"The linguistic scrutiny of any aspect of pragmatics is bound to considering carefully the context in which it is expressed. This becomes very glaring when the utterances that constitute discourse are viewed as performing social actions. In an EFL context like Iran, English language learners are in urgent need of raising their pragmatic awareness as they do not have regular access to an environment where the main language of communication is English. The major context for improving their pragmatic competence could be classroom. Thus, teachers need to cope with this issue which makes knowledge of speech acts more worthy of being dealt with. This research was an attempt to find out the effect of teacher’s group feedback versus individual feedback on Iranian EFL learners’ uptake of speech acts. To carry out the study, 32 pre-intermediate learners were chosen to take part based on convenient sampling through non-random grouping. To make sure that the learners were homogeneous, a Quick Placement Test (QPT) was administered before the treatment. Based on the design of the study, the learners were divided into two treatment conditions of individual and group feedback. The research was a quasi-experimental a pretest and a posttest of writing before and after the treatment to measure students’ uptake of speech acts. Results of two paired-samples t-test and an independent samples t-test showed significant effect of individual feedback on learners’ uptake of speech acts, while group feedback was not significant. It was also found that there was a significant difference between the effects of teacher feedback in groups vs. teacher feedback given to individual learners. The present research has significant implications. Tests can also focus more on promoting individual feedback among learners and teachers in washback effect. Teachers can also make principled decisions about the feedback condition that best boost leaner uptake in speech act."
43,"
An approach to automatic classification of hate speech in sports domain on social media
",Journal of Big Data,N/A,Staša Vujičić Stanković and Miljana Mladenović,22 June 2023,https://journalofbigdata.springeropen.com/articles/10.1186/s40537-023-00766-9,https://journalofbigdata.springeropen.com/counter/pdf/10.1186/s40537-023-00766-9.pdf,"Hate Speech encompasses different forms of trolling, bullying, harassment, and threats directed against specific individuals or groups. This phenomena is mainly expressed on Social Networks. For sports players, Social Media is a means of communication with the widest part of their fans and a way to face different cyber-aggression forms. These virtual attacks can harm players, distress them, cause them to feel bad for a long time, or even escalate into physical violence. To date, athletes were not observed as a vulnerable group, so they were not a subject of automatic Hate Speech detection and recognition from content published on Social Media. This paper explores whether a model trained on the dataset from one Social Media and not related to any specific domain can be efficient for the Hate Speech binary classification of test sets regarding the sports domain. The experiments deal with Hate Speech detection in Serbian. BiLSTM deep neural network was learned with different parameters, and the results showed high Precision of detecting Hate Speech in sports domain (96% and 97%) and pretty low Recall."
44,"
Pronunciation augmentation for Mandarin-English code-switching speech recognition
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Yanhua Long, Shuang Wei, Jie Lian and Yijie Li",30 August 2021,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-021-00222-7,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-021-00222-7.pdf,"Code-switching (CS) refers to the phenomenon of using more than one language in an utterance, and it presents great challenge to automatic speech recognition (ASR) due to the code-switching property in one utterance, the pronunciation variation phenomenon of the embedding language words and the heavy training data sparse problem. This paper focuses on the Mandarin-English CS ASR task. We aim at dealing with the pronunciation variation and alleviating the sparse problem of code-switches by using pronunciation augmentation methods. An English-to-Mandarin mix-language phone mapping approach is first proposed to obtain a language-universal CS lexicon. Based on this lexicon, an acoustic data-driven lexicon learning framework is further proposed to learn new pronunciations to cover the accents, mis-pronunciations, or pronunciation variations of those embedding English words. Experiments are performed on real CS ASR tasks. Effectiveness of the proposed methods are examined on all of the conventional, hybrid, and the recent end-to-end speech recognition systems. Experimental results show that both the learned phone mapping and augmented pronunciations can significantly improve the performance of code-switching speech recognition."
45,"
Segment boundary detection directed attention for online end-to-end speech recognition
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Junfeng Hou, Wu Guo, Yan Song and Li-Rong Dai",30 January 2020,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-020-0170-z,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-020-0170-z.pdf,"Attention-based encoder-decoder models have recently shown competitive performance for automatic speech recognition (ASR) compared to conventional ASR systems. However, how to employ attention models for online speech recognition still needs to be explored. Different from conventional attention models wherein the soft alignment is obtained by a pass over the entire input sequence, attention models for online recognition must learn online alignment to attend part of input sequence monotonically when generating output symbols. Based on the fact that every output symbol is corresponding to a segment of input sequence, we propose a new attention mechanism for learning online alignment by decomposing the conventional alignment into two parts: segmentation—segment boundary detection with hard decision—and segment-directed attention—information aggregation within the segment with soft attention. The boundary detection is conducted along the time axis from left to right, and a decision is made for each input frame about whether it is a segment boundary or not. When a boundary is detected, the decoder generates an output symbol by attending the inputs within the corresponding segment. With the proposed attention mechanism, online speech recognition can be realized. The experimental results on TIMIT and WSJ dataset show that our proposed attention mechanism achieves comparable online performance with state-of-the-art models."
46,"
End-to-end speech emotion recognition using a novel context-stacking dilated convolution neural network
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Duowei Tang, Peter Kuppens, Luc Geurts and Toon van Waterschoot",12 May 2021,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-021-00208-5,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-021-00208-5.pdf,"Amongst the various characteristics of a speech signal, the expression of emotion is one of the characteristics that exhibits the slowest temporal dynamics. Hence, a performant speech emotion recognition (SER) system requires a predictive model that is capable of learning sufficiently long temporal dependencies in the analysed speech signal. Therefore, in this work, we propose a novel end-to-end neural network architecture based on the concept of dilated causal convolution with context stacking. Firstly, the proposed model consists only of parallelisable layers and is hence suitable for parallel processing, while avoiding the inherent lack of parallelisability occurring with recurrent neural network (RNN) layers. Secondly, the design of a dedicated dilated causal convolution block allows the model to have a receptive field as large as the input sequence length, while maintaining a reasonably low computational cost. Thirdly, by introducing a context stacking structure, the proposed model is capable of exploiting long-term temporal dependencies hence providing an alternative to the use of RNN layers. We evaluate the proposed model in SER regression and classification tasks and provide a comparison with a state-of-the-art end-to-end SER model. Experimental results indicate that the proposed model requires only 1/3 of the number of model parameters used in the state-of-the-art model, while also significantly improving SER performance. Further experiments are reported to understand the impact of using various types of input representations (i.e. raw audio samples vs log mel-spectrograms) and to illustrate the benefits of an end-to-end approach over the use of hand-crafted audio features. Moreover, we show that the proposed model can efficiently learn intermediate embeddings preserving speech emotion information."
47,"
Significance of parametric spectral ratio methods in detection and recognition of whispered speech
",EURASIP Journal on Advances in Signal Processing,N/A,"Arpit Mathur, Shankar M Reddy and Rajesh M Hegde",23 July 2012,https://asp-eurasipjournals.springeropen.com/articles/10.1186/1687-6180-2012-157,https://asp-eurasipjournals.springeropen.com/counter/pdf/10.1186/1687-6180-2012-157.pdf,"In this article the significance of a new parametric spectral ratio method that can be used to detect whispered speech segments within normally phonated speech is described. Adaptation methods based on the maximum likelihood linear regression (MLLR) are then used to realize a mismatched train-test style speech recognition system. This proposed parametric spectral ratio method computes a ratio spectrum of the linear prediction (LP) and the minimum variance distortion-less response (MVDR) methods. The smoothed ratio spectrum is then used to detect whispered segments of speech within neutral speech segments effectively. The proposed LP-MVDR ratio method exhibits robustness at different SNRs as indicated by the whisper diarization experiments conducted on the CHAINS and the cell phone whispered speech corpus. The proposed method also performs reasonably better than the conventional methods for whisper detection. In order to integrate the proposed whisper detection method into a conventional speech recognition engine with minimal changes, adaptation methods based on the MLLR are used herein. The hidden Markov models corresponding to neutral mode speech are adapted to the whispered mode speech data in the whispered regions as detected by the proposed ratio method. The performance of this method is first evaluated on whispered speech data from the CHAINS corpus. The second set of experiments are conducted on the cell phone corpus of whispered speech. This corpus is collected using a set up that is used commercially for handling public transactions. The proposed whisper speech recognition system exhibits reasonably better performance when compared to several conventional methods. The results shown indicate the possibility of a whispered speech recognition system for cell phone based transactions."
48,"
Microphone array power ratio for quality assessment of reverberated speech
",EURASIP Journal on Advances in Signal Processing,N/A,Reuven Berkun and Israel Cohen,18 June 2015,https://asp-eurasipjournals.springeropen.com/articles/10.1186/s13634-015-0233-y,https://asp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13634-015-0233-y.pdf,"Speech signals in enclosed environments are often distorted by reverberation and noise. In speech communication systems with several randomly distributed microphones, involving a dynamic speaker and unknown source location, it is of great interest to monitor the perceived quality at each microphone and select the signal with the best quality. Most of existing approaches for quality estimation require prior information or a clean reference signal, which is unfortunately seldom available. In this paper, a practical non-intrusive method for quality assessment of reverberated speech signals is proposed. Using a statistical model of the reverberation process, we examine the energies as measured by unidirectional elements in a microphone array. By measuring the power ratio, we obtain a measure for the amount of reverberation in the received acoustic signals. This measure is then utilized to derive a blind estimation of the direct-to-reverberation energy ratio in the room. The proposed approach attains a simple, reliable, and robust quality measure, shown here through persuasive simulation results."
49,"
Neural network-based non-intrusive speech quality assessment using attention pooling function
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Miao Liu, Jing Wang, Weiming Yi and Fang Liu",17 May 2021,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-021-00209-4,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-021-00209-4.pdf,"Recently, the non-intrusive speech quality assessment method has attracted a lot of attention since it does not require the original reference signals. At the same time, neural networks began to be applied to speech quality assessment and achieved good performance. To improve the performance of non-intrusive speech quality assessment, this paper proposes a neural network-based assessment method using attention pooling function. The proposed systems are based on the convolutional neural networks (CNNs), bidirectional long short-term memory (BLSTM), and CNN-LSTM structure. Comparing four types of pooling functions both theoretically and experimentally, we find the attention pooling function performs the best among the four. Experiments are conducted in a dataset containing various degraded speech signals with corresponding subjective quality scores. The results show that the proposed CNN-LSTM model using attention pooling function achieves state-of-the-art correlation coefficient (R) and root-mean-square error (RMSE) of 0.967 and 0.269, outperforming the performance of standardization ITU-T P.563 and autoencoder-support vector regression method."
50,"
Speaker-dependent model interpolation for statistical emotional speech synthesis
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,Chih-Yu Hsu and Chia-Ping Chen,16 August 2012,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/1687-4722-2012-21,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/1687-4722-2012-21.pdf,"In this article, we propose a speaker-dependent model interpolation method for statistical emotional speech synthesis. The basic idea is to combine the neutral model set of the target speaker and an emotional model set selected from a pool of speakers. For model selection and interpolation weight determination, we propose to use a novel monophone-based Mahalanobis distance, which is a proper distance measure between two Hidden Markov Model sets. We design Latin-square evaluation to reduce the systematic bias in the subjective listening tests. The proposed interpolation method achieves sound performance on the emotional expressiveness, the naturalness, and the target speaker similarity. Moreover, such performance is achieved without the need to collect the emotional speech of the target speaker, saving the cost of data collection and labeling."
51,"
Impact and dynamics of hate and counter speech online
",EPJ Data Science,N/A,"Joshua Garland, Keyan Ghazi-Zahedi, Jean-Gabriel Young, Laurent Hébert-Dufresne and Mirta Galesic",24 January 2022,https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-021-00314-6,https://epjdatascience.springeropen.com/counter/pdf/10.1140/epjds/s13688-021-00314-6.pdf,"Citizen-generated counter speech is a promising way to fight hate speech and promote peaceful, non-polarized discourse. However, there is a lack of large-scale longitudinal studies of its effectiveness for reducing hate speech. To this end, we perform an exploratory analysis of the effectiveness of counter speech using several different macro- and micro-level measures to analyze 131,366 political conversations that took place on German Twitter over four years. We report on the dynamic interactions of hate and counter speech over time and provide insights into whether, as in ‘classic’ bullying situations, organized efforts are more effective than independent individuals in steering online discourse. Taken together, our results build a multifaceted picture of the dynamics of hate and counter speech online. While we make no causal claims due to the complexity of discourse dynamics, our findings suggest that organized hate speech is associated with changes in public discourse and that counter speech—especially when organized—may help curb hateful rhetoric in online discourse."
52,"
Data-Model Relationship in Text-Independent Speaker Recognition
",EURASIP Journal on Advances in Signal Processing,N/A,"John S. D. Mason, Nicholas W. D. Evans, Robert Stapert and Roland Auckenthaler",30 March 2005,https://asp-eurasipjournals.springeropen.com/articles/10.1155/ASP.2005.471,https://asp-eurasipjournals.springeropen.com/counter/pdf/10.1155/ASP.2005.471.pdf,"Text-independent speaker recognition systems such as those based on Gaussian mixture models (GMMs) do not include time sequence information (TSI) within the model itself. The level of importance of TSI in speaker recognition is an interesting question and one addressed in this paper. Recent works has shown that the utilisation of higher-level information such as idiolect, pronunciation, and prosodics can be useful in reducing speaker recognition error rates. In accordance with these developments, the aim of this paper is to show that as more data becomes available, the basic GMM can be enhanced by utilising TSI, even in a text-independent mode. This paper presents experimental work incorporating TSI into the conventional GMM. The resulting system, known as the segmental mixture model (SMM), embeds dynamic time warping (DTW) into a GMM framework. Results are presented on the 2000-speaker SpeechDat Welsh database which show improved speaker recognition performance with the SMM."
53,"
A study on the consistency analysis of energy parameter for Mandarin speech
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Li-Te Shen, Cheng-Yu Yeh and Shaw-Hwa Hwang",17 December 2012,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/1687-4722-2012-28,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/1687-4722-2012-28.pdf,"In this study, a consistency analysis of energy parameter for Mandarin speech is presented. Identified as a result of inspection of the human pronunciation process, the consistency can be interpreted as a high correlation of a warping curve between the spectrum and the prosody intra a syllable. Through three steps in the procedure of the consistency analysis, the hidden Markov model (HMM) algorithm is used first to decode HMM-state sequences within a syllable at the same time as to divide them into three segments. Second, based on a designated syllable, the vector quantization (VQ) with the Linde–Buzo–Gray algorithm is used to train the VQ codebooks of each segment. Third, the energy vector of each segment is encoded as an index by VQ codebooks, and then the probability of each possible path is evaluated as a prerequisite to analyze the consistency. It is demonstrated experimentally that a consistency is definitely acquired in case the syllable is located exactly in the same word. These results offer a research direction that the energy warping process intra a syllable must be considered in a text-to-speech system to improve the synthesized speech quality."
54,"
Bayesian STSA estimation using masking properties and generalized Gamma prior for speech enhancement
",EURASIP Journal on Advances in Signal Processing,N/A,"Mahdi Parchami, Wei-Ping Zhu, Benoit Champagne and Eric Plourde",6 October 2015,https://asp-eurasipjournals.springeropen.com/articles/10.1186/s13634-015-0270-6,https://asp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13634-015-0270-6.pdf,"We consider the estimation of the speech short-time spectral amplitude (STSA) using a parametric Bayesian cost function and speech prior distribution. First, new schemes are proposed for the estimation of the cost function parameters, using an initial estimate of the speech STSA along with the noise masking feature of the human auditory system. This information is further employed to derive a new technique for the gain flooring of the STSA estimator. Next, to achieve better compliance with the noisy speech in the estimator’s gain function, we take advantage of the generalized Gamma distribution in order to model the STSA prior and propose an SNR-based scheme for the estimation of its corresponding parameters. It is shown that in Bayesian STSA estimators, the exploitation of a rough STSA estimate in the parameter selection for the cost function and the speech prior leads to more efficient control on the gain function values. Performance evaluation in different noisy scenarios demonstrates the superiority of the proposed methods over the existing parametric STSA estimators in terms of the achieved noise reduction and introduced speech distortion."
55,"
Empirically combining unnormalized NNLM and back-off N-gram for fast N-best rescoring in speech recognition
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Yongzhe Shi, Wei-Qiang Zhang, Meng Cai and Jia Liu",28 April 2014,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/1687-4722-2014-19,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/1687-4722-2014-19.pdf,"Neural network language models (NNLM) have been proved to be quite powerful for sequence modeling, including feed-forward NNLM (FNNLM), recurrent NNLM (RNNLM), etc. One main issue concerned for NNLM is the heavy computational burden of the output layer, where the output needs to be probabilistically normalized and the normalizing factors require lots of computation. How to fast rescore the N-best list or lattice with NNLM attracts much attention for large-scale applications. In this paper, the statistic characteristics of normalizing factors are investigated on the N-best list. Based on the statistic observations, we propose to approximate the normalizing factors for each hypothesis as a constant proportional to the number of words in the hypothesis. Then, the unnormalized NNLM is investigated and combined with back-off N-gram for fast rescoring, which can be computed very fast without the normalization in the output layer, with the complexity reduced significantly. We apply our proposed method to a well-tuned context-dependent deep neural network hidden Markov model (CD-DNN-HMM) speech recognition system on the English-Switchboard phone-call speech-to-text task, where both FNNLM and RNNLM are trained to demonstrate our method. Experimental results show that unnormalized probability of NNLM is quite complementary to that of back-off N-gram, and combining the unnormalized NNLM and back-off N-gram can further reduce the word error rate with little computational consideration."
56,"
Time-domain adaptive attention network for single-channel speech separation
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Kunpeng Wang, Hao Zhou, Jingxiang Cai, Wenna Li and Juan Yao",11 May 2023,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-023-00283-w,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-023-00283-w.pdf,"Recent years have witnessed a great progress in single-channel speech separation by applying self-attention based networks. Despite the excellent performance in mining relevant long-sequence contextual information, self-attention networks cannot perfectly focus on subtle details in speech signals, such as temporal or spectral continuity, spectral structure, and timbre. To tackle this problem, we proposed a time-domain adaptive attention network (TAANet) with local and global attention network. Channel and spatial attention are introduced in local attention networks to focus on subtle details of the speech signals (frame-level features). In the global attention networks, a self-attention mechanism is used to explore the global associations of the speech contexts (utterance-level features). Moreover, we model the speech signal serially using multiple local and global attention blocks. This cascade structure enables our model to focus on local and global features adaptively, compared with other speech separation feature extraction methods, further boosting the separation performance. Versus other end-to-end speech separation methods, extensive experiments on benchmark datasets demonstrate that our approach obtains a superior result. (20.7 dB of SI-SNRi and 20.9 dB of SDRi on WSJ0-2mix)."
57,"
Evaluation of the effectiveness and efficiency of state-of-the-art features and models for automatic speech recognition error detection
",Journal of Big Data,N/A,"Asmaa El Hannani, Rahhal Errattahi, Fatima Zahra Salmam, Thomas Hain and Hassan Ouahmane",6 January 2021,https://journalofbigdata.springeropen.com/articles/10.1186/s40537-020-00391-w,https://journalofbigdata.springeropen.com/counter/pdf/10.1186/s40537-020-00391-w.pdf,"Speech based human-machine interaction and natural language understanding applications have seen a rapid development and wide adoption over the last few decades. This has led to a proliferation of studies that investigate Error detection and classification in Automatic Speech Recognition (ASR) systems. However, different data sets and evaluation protocols are used, making direct comparisons of the proposed approaches (e.g. features and models) difficult. In this paper we perform an extensive evaluation of the effectiveness and efficiency of state-of-the-art approaches in a unified framework for both errors detection and errors type classification. We make three primary contributions throughout this paper: (1) we have compared our Variant Recurrent Neural Network (V-RNN) model with three other state-of-the-art neural based models, and have shown that the V-RNN model is the most effective classifier for ASR error detection in term of accuracy and speed, (2) we have compared four features’ settings, corresponding to different categories of predictor features and have shown that the generic features are particularly suitable for real-time ASR error detection applications, and (3) we have looked at the post generalization ability of our error detection framework and performed a detailed post detection analysis in order to perceive the recognition errors that are difficult to detect."
58,"
Black-box adversarial attacks through speech distortion for speech emotion recognition
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Jinxing Gao, Diqun Yan and Mingyu Dong",17 August 2022,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-022-00254-7,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-022-00254-7.pdf,"Speech emotion recognition is a key branch of affective computing. Nowadays, it is common to detect emotional diseases through speech emotion recognition. Various detection methods of emotion recognition, such as LTSM, GCN, and CNN, show excellent performance. However, due to the robustness of the model, the recognition results of the above models will have a large deviation. So in this article, we use black boxes to combat sample attacks to explore the robustness of the model. After using three different black-box attacks, the accuracy of the CNN-MAA model decreased by 69.38% at the best attack scenario, while the word error rate (WER) of voice decreased by only 6.24%, indicating that the robustness of the model does not perform well under our black-box attack method. After adversarial training, the model accuracy only decreased by 13.48%, which shows the effectiveness of adversarial training against sample attacks. Our code is available in Github."
59,"
Within and cross-corpus speech emotion recognition using latent topic model-based features
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Mohit Shah, Chaitali Chakrabarti and Andreas Spanias",25 January 2015,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-014-0049-y,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-014-0049-y.pdf,"Owing to the suprasegmental behavior of emotional speech, turn-level features have demonstrated a better success than frame-level features for recognition-related tasks. Conventionally, such features are obtained via a brute-force collection of statistics over frames, thereby losing important local information in the process which affects the performance. To overcome these limitations, a novel feature extraction approach using latent topic models (LTMs) is presented in this study. Speech is assumed to comprise of a mixture of emotion-specific topics, where the latter capture emotionally salient information from the co-occurrences of frame-level acoustic features and yield better descriptors. Specifically, a supervised replicated softmax model (sRSM), based on restricted Boltzmann machines and distributed representations, is proposed to learn naturally discriminative topics. The proposed features are evaluated for the recognition of categorical or continuous emotional attributes via within and cross-corpus experiments conducted over acted and spontaneous expressions. In a within-corpus scenario, sRSM outperforms competing LTMs, while obtaining a significant improvement of 16.75% over popular statistics-based turn-level features for valence-based classification, which is considered to be a difficult task using only speech. Further analyses with respect to the turn duration show that the improvement is even more significant, 35%, on longer turns (>6 s), which is highly desirable for current turn-based practices. In a cross-corpus scenario, two novel adaptation-based approaches, instance selection, and weight regularization are proposed to reduce the inherent bias due to varying annotation procedures and cultural perceptions across databases. Experimental results indicate a natural, yet less severe, deterioration in performance - only 2.6% and 2.7%, thereby highlighting the generalization ability of the proposed features."
60,"
Language Model Adaptation Using Machine-Translated Text for Resource-Deficient Languages
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"ArnarThor Jensson, Koji Iwano and Sadaoki Furui",27 January 2009,https://asmp-eurasipjournals.springeropen.com/articles/10.1155/2008/573832,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1155/2008/573832.pdf,"Text corpus size is an important issue when building a language model (LM). This is a particularly important issue for languages where little data is available. This paper introduces an LM adaptation technique to improve an LM built using a small amount of task-dependent text with the help of a machine-translated text corpus. Icelandic speech recognition experiments were performed using data, machine translated (MT) from English to Icelandic on a word-by-word and sentence-by-sentence basis. LM interpolation using the baseline LM and an LM built from either word-by-word or sentence-by-sentence translated text reduced the word error rate significantly when manually obtained utterances used as a baseline were very sparse."
61,"
Feature trajectory dynamic time warping for clustering of speech segments
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,Lerato Lerato and Thomas Niesler,4 April 2019,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-019-0149-9,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-019-0149-9.pdf,"Dynamic time warping (DTW) can be used to compute the similarity between two sequences of generally differing length. We propose a modification to DTW that performs individual and independent pairwise alignment of feature trajectories. The modified technique, termed feature trajectory dynamic time warping (FTDTW), is applied as a similarity measure in the agglomerative hierarchical clustering of speech segments. Experiments using MFCC and PLP parametrisations extracted from TIMIT and from the Spoken Arabic Digit Dataset (SADD) show consistent and statistically significant improvements in the quality of the resulting clusters in terms of F-measure and normalised mutual information (NMI)."
62,"
Evaluating word embeddings and a revised corpus for part-of-speech tagging in Portuguese
",Journal of the Brazilian Computer Society,N/A,"Erick R Fonseca, João Luís G Rosa and Sandra Maria Aluísio",6 February 2015,https://journal-bcs.springeropen.com/articles/10.1186/s13173-014-0020-x,https://journal-bcs.springeropen.com/counter/pdf/10.1186/s13173-014-0020-x.pdf,"Part-of-speech tagging is an important preprocessing step in many natural language processing applications. Despite much work already carried out in this field, there is still room for improvement, especially in Portuguese. We experiment here with an architecture based on neural networks and word embeddings, and that has achieved promising results in English."
63,"
Towards the universal defense for query-based audio adversarial attacks on speech recognition system
",Cybersecurity,N/A,"Feng Guo, Zheng Sun, Yuxuan Chen and Lei Ju",5 August 2023,https://cybersecurity.springeropen.com/articles/10.1186/s42400-023-00177-6,https://cybersecurity.springeropen.com/counter/pdf/10.1186/s42400-023-00177-6.pdf,"Recently, studies show that deep learning-based automatic speech recognition (ASR) systems are vulnerable to adversarial examples (AEs), which add a small amount of noise to the original audio examples. These AE attacks pose new challenges to deep learning security and have raised significant concerns about deploying ASR systems and devices. The existing defense methods are either limited in application or only defend on results, but not on process. In this work, we propose a novel method to infer the adversary intent and discover audio adversarial examples based on the AEs generation process. The insight of this method is based on the observation: many existing audio AE attacks utilize query-based methods, which means the adversary must send continuous and similar queries to target ASR models during the audio AE generation process. Inspired by this observation, We propose a memory mechanism by adopting audio fingerprint technology to analyze the similarity of the current query with a certain length of memory query. Thus, we can identify when a sequence of queries appears to be suspectable to generate audio AEs. Through extensive evaluation on four state-of-the-art audio AE attacks, we demonstrate that on average our defense identify the adversary’s intent with over \(90\%\) accuracy. With careful regard for robustness evaluations, we also analyze our proposed defense and its strength to withstand two adaptive attacks. Finally, our scheme is available out-of-the-box and directly compatible with any ensemble of ASR defense models to uncover audio AE attacks effectively without model retraining."
64,"
Features for voice activity detection: a comparative analysis
",EURASIP Journal on Advances in Signal Processing,N/A,"Simon Graf, Tobias Herbig, Markus Buck and Gerhard Schmidt",11 November 2015,https://asp-eurasipjournals.springeropen.com/articles/10.1186/s13634-015-0277-z,https://asp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13634-015-0277-z.pdf,"In many speech signal processing applications, voice activity detection (VAD) plays an essential role for separating an audio stream into time intervals that contain speech activity and time intervals where speech is absent. Many features that reflect the presence of speech were introduced in literature. However, to our knowledge, no extensive comparison has been provided yet. In this article, we therefore present a structured overview of several established VAD features that target at different properties of speech. We categorize the features with respect to properties that are exploited, such as power, harmonicity, or modulation, and evaluate the performance of some dedicated features. The importance of temporal context is discussed in relation to latency restrictions imposed by different applications. Our analyses allow for selecting promising VAD features and finding a reasonable trade-off between performance and complexity."
65,"
Deep neural networks for automatic speech processing: a survey from large corpora to limited data
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Vincent Roger, Jérôme Farinas and Julien Pinquier",17 August 2022,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-022-00251-w,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-022-00251-w.pdf,"Most state-of-the-art speech systems use deep neural networks (DNNs). These systems require a large amount of data to be learned. Hence, training state-of-the-art frameworks on under-resourced speech challenges are difficult tasks. As an example, a challenge could be the limited amount of data to model impaired speech. Furthermore, acquiring more data and/or expertise is time-consuming and expensive. In this paper, we focus on the following speech processing tasks: automatic speech recognition, speaker identification, and emotion recognition. To assess the problem of limited data, we firstly investigate state-of-the-art automatic speech recognition systems, as this is the hardest task (due to the wide variability in each language). Next, we provide an overview of techniques and tasks requiring fewer data. In the last section, we investigate few-shot techniques by interpreting under-resourced speech as a few-shot problem. In that sense, we propose an overview of few-shot techniques and the possibility of using such techniques for the speech problems addressed in this survey. It is true that the reviewed techniques are not well adapted for large datasets. Nevertheless, some promising results from the literature encourage the usage of such techniques for speech processing."
66,"
Quadratic approach for single-channel noise reduction
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Gal Itzhak, Jacob Benesty and Israel Cohen",15 April 2020,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-020-00174-4,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-020-00174-4.pdf,"In this paper, we introduce a quadratic approach for single-channel noise reduction. The desired signal magnitude is estimated by applying a linear filter to a modified version of the observations’ vector. The modified version is constructed from a Kronecker product of the observations’ vector with its complex conjugate. The estimated signal magnitude is multiplied by a complex exponential whose phase is obtained using a conventional linear filtering approach. We focus on the linear and quadratic maximum signal-to-noise ratio (SNR) filters and demonstrate that the quadratic filter is superior in terms of subband SNR gains. In addition, in the context of speech enhancement, we show that the quadratic filter is ideally preferable in terms of perceptual evaluation of speech quality (PESQ) and short-time objective intelligibility (STOI) scores. The advantages, compared to the conventional linear filtering approach, are particularly significant for low input SNRs, at the expanse of a higher computational complexity. The results are verified in practical scenarios with nonstationary noise and in comparison to well-known speech enhancement methods. We demonstrate that the quadratic maximum SNR filter may be superior, depending on the nonstationary noise type."
67,"
Audio-Visual Speech Recognition Using MPEG-4 Compliant Visual Features
",EURASIP Journal on Advances in Signal Processing,N/A,"Petar S. Aleksic, Jay J. Williams, Zhilin Wu and Aggelos K. Katsaggelos",28 November 2002,https://asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865702206162,https://asp-eurasipjournals.springeropen.com/counter/pdf/10.1155/S1110865702206162.pdf,"We describe an audio-visual automatic continuous speech recognition system, which significantly improves speech recognition performance over a wide range of acoustic noise levels, as well as under clean audio conditions. The system utilizes facial animation parameters (FAPs) supported by the MPEG-4 standard for the visual representation of speech. We also describe a robust and automatic algorithm we have developed to extract FAPs from visual data, which does not require hand labeling or extensive training procedures. The principal component analysis (PCA) was performed on the FAPs in order to decrease the dimensionality of the visual feature vectors, and the derived projection weights were used as visual features in the audio-visual automatic speech recognition (ASR) experiments. Both single-stream and multistream hidden Markov models (HMMs) were used to model the ASR system, integrate audio and visual information, and perform a relatively large vocabulary (approximately 1000 words) speech recognition experiments. The experiments performed use clean audio data and audio data corrupted by stationary white Gaussian noise at various SNRs. The proposed system reduces the word error rate (WER) by 20% to 23% relatively to audio-only speech recognition WERs, at various SNRs (0–30 dB) with additive white Gaussian noise, and by 19% relatively to audio-only speech recognition WER under clean audio conditions."
68,"
An examination of Vygotsky’s socio-cultural theory in second language acquisition: the role of higher order thinking enhancing techniques and the EFL learners’ use of private speech in the construction of reasoning
",Asian-Pacific Journal of Second and Foreign Language Education,N/A,Sepideh Mirzaee and Parviz Maftoon,6 December 2016,https://sfleducation.springeropen.com/articles/10.1186/s40862-016-0022-7,https://sfleducation.springeropen.com/counter/pdf/10.1186/s40862-016-0022-7.pdf,"The present study investigates the impact of higher order thinking enhancing techniques as two post-reading strategies on the EFL students’ reasoning power as determined by their private speech production. Also, the relationship between the learners’ private speech production and their reasoning power is also investigated. In so doing, the study utilizes a quasi-experimental design with 30 participants in each control and experimental groups. The results of the pretest administration indicate that the participants of the two groups are homogenous regarding their language proficiency level as determined by the Babel Test, and their reasoning power as determined by the Watson-Glaser Critical Thinking Appraisal (W-GCTA). The participants in the experimental group are instructed in a way to enhance their reasoning power based on 6 reading comprehension texts. WGCTA’s two subtests of deduction and inference making which contain reasoning-gap tasks are utilized as the posttest. While the participants are engaged in carrying out such tasks, their private speech productions are recorded. Data analysis and transcription indicate that private speech which is categorized into 4 classes in this study has a positive and significant influence on the learners’ reasoning power. Higher order thinking enhancing techniques are also found to have impact on the experimental group in enhancing their private speech production and subsequently improving their reasoning. The detailed results, discussion, and conclusions of the research are further presented."
69,"
An acoustic echo canceller optimized for hands-free speech telecommunication in large vehicle cabins
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Amin Saremi, Balaji Ramkumar, Ghazaleh Ghaffari and Zonghua Gu",7 October 2023,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-023-00305-7,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-023-00305-7.pdf,"Acoustic echo cancelation (AEC) is a system identification problem that has been addressed by various techniques and most commonly by normalized least mean square (NLMS) adaptive algorithms. However, performing a successful AEC in large commercial vehicles has proved complicated due to the size and challenging variations in the acoustic characteristics of their cabins. Here, we present a wideband fully linear time domain NLMS algorithm for AEC that is enhanced by a statistical double-talk detector (DTD) and a voice activity detector (VAD). The proposed solution was tested in four main Volvo truck models, with various cabin geometries, using standard Swedish hearing-in-noise (HINT) sentences in the presence and absence of engine noise. The results show that the proposed solution achieves a high echo return loss enhancement (ERLE) of at least 25 dB with a fast convergence time, fulfilling ITU G.168 requirements. The presented solution was particularly developed to provide a practical compromise between accuracy and computational cost to allow its real-time implementation on commercial digital signal processors (DSPs). A real-time implementation of the solution was coded in C on an ARM Cortex M-7 DSP. The algorithmic latency was measured at less than 26 ms for processing each 50-ms buffer indicating the computational feasibility of the proposed solution for real-time implementation on common DSPs and embedded systems with limited computational and memory resources. MATLAB source codes and related audio files are made available online for reference and further development."
70,"
Audiovisual Speech Synchrony Measure: Application to Biometrics
",EURASIP Journal on Advances in Signal Processing,N/A,Hervé Bredin and Gérard Chollet,1 December 2007,https://asp-eurasipjournals.springeropen.com/articles/10.1155/2007/70186,https://asp-eurasipjournals.springeropen.com/counter/pdf/10.1155/2007/70186.pdf,"Speech is a means of communication which is intrinsically bimodal: the audio signal originates from the dynamics of the articulators. This paper reviews recent works in the field of audiovisual speech, and more specifically techniques developed to measure the level of correspondence between audio and visual speech. It overviews the most common audio and visual speech front-end processing, transformations performed on audio, visual, or joint audiovisual feature spaces, and the actual measure of correspondence between audio and visual speech. Finally, the use of synchrony measure for biometric identity verification based on talking faces is experimented on the BANCA database."
71,"
Low-complexity artificial noise suppression methods for deep learning-based speech enhancement algorithms
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Yuxuan Ke, Andong Li, Chengshi Zheng, Renhua Peng and Xiaodong Li",12 April 2021,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-021-00204-9,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-021-00204-9.pdf,"Deep learning-based speech enhancement algorithms have shown their powerful ability in removing both stationary and non-stationary noise components from noisy speech observations. But they often introduce artificial residual noise, especially when the training target does not contain the phase information, e.g., ideal ratio mask, or the clean speech magnitude and its variations. It is well-known that once the power of the residual noise components exceeds the noise masking threshold of the human auditory system, the perceptual speech quality may degrade. One intuitive way is to further suppress the residual noise components by a postprocessing scheme. However, the highly non-stationary nature of this kind of residual noise makes the noise power spectral density (PSD) estimation a challenging problem. To solve this problem, the paper proposes three strategies to estimate the noise PSD frame by frame, and then the residual noise can be removed effectively by applying a gain function based on the decision-directed approach. The objective measurement results show that the proposed postfiltering strategies outperform the conventional postfilter in terms of segmental signal-to-noise ratio (SNR) as well as speech quality improvement. Moreover, the AB subjective listening test shows that the preference percentages of the proposed strategies are over 60%."
72,"
Classification-based spoken text selection for LVCSR language modeling
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,Vataya Chunwijitra and Chai Wutiwiwatchai,17 October 2017,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-017-0121-5,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-017-0121-5.pdf,"Large vocabulary continuous speech recognition (LVCSR) has naturally been demanded for transcribing daily conversations, while developing spoken text data to train LVCSR is costly and time-consuming. In this paper, we propose a classification-based method to automatically select social media data for constructing a spoken-style language model in LVCSR. Three classification techniques, SVM, CRF, and LSTM, trained by words and parts-of-speech are comparatively experimented to identify the degree of spoken style in each social media sentence. Spoken-style utterances are chosen by incremental greedy selection based on the score of the SVM or the CRF classifier or the output classified as “spoken” by the LSTM classifier. With the proposed method, just 51.8, 91.6, and 79.9% of the utterances in a Twitter text collection are marked as spoken utterances by the SVM, CRF, and LSTM classifiers, respectively. A baseline language model is then improved by interpolating with the one trained by these selected utterances. The proposed model is evaluated on two Thai LVCSR tasks: social media conversations and a speech-to-speech translation application. Experimental results show that all the three classification-based data selection methods clearly help reducing the overall spoken test set perplexities. Regarding the LVCSR word error rate (WER), they achieve 3.38, 3.44, and 3.39% WER reduction, respectively, over the baseline language model, and 1.07, 0.23, and 0.38% WER reduction, respectively, over the conventional perplexity-based text selection approach."
73,"
Prosodic mapping of text font based on the dimensional theory of emotions: a case study on style and size
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,Dimitrios Tsonos and Georgios Kouroupetroglou,15 March 2016,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-016-0087-8,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-016-0087-8.pdf,"Current text-to-speech systems do not support the effective provision of the semantics and the cognitive aspects of the documents’ typographic cues (e.g., font type, style, and size). A novel approach is introduced for the acoustic rendition of text font based on the emotional analogy between the visual (text font cues) and the acoustic (speech prosody) modalities. The methodology is based on: a) modeling reader’s emotional state response (“Pleasure”, “Arousal” and “Dominance”) induced by the document’s font cues and b) the acoustic mapping of the emotional state using expressive speech synthesis. A case study was conducted for the proposed methodology by calculating the prosodic values on specific font cues (several font styles and font sizes) and by examining listeners’ preferences on the acoustic rendition of bold, italics, bold-italics, and various font sizes. The experimental results after the user evaluation indicate that the acoustic rendition of font size variations as well as bold and italics is recognized successfully, but bold-italics are confused with bold, due to the similarities of their prosodic variations."
74,"
Developing a unit selection voice given audio without corresponding text
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Tejas Godambe, Sai Krishna Rallabandi, Suryakanth V. Gangashetty, Ashraf Alkhairy and Afshan Jafri",1 March 2016,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-016-0084-y,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-016-0084-y.pdf,"Today, a large amount of audio data is available on the web in the form of audiobooks, podcasts, video lectures, video blogs, news bulletins, etc. In addition, we can effortlessly record and store audio data such as a read, lecture, or impromptu speech on handheld devices. These data are rich in prosody and provide a plethora of voices to choose from, and their availability can significantly reduce the overhead of data preparation and help rapid building of synthetic voices. But, a few problems are associated with readily using this data such as (1) these audio files are generally long, and audio-transcription alignment is memory intensive; (2) precise corresponding transcriptions are unavailable, (3) many times, no transcriptions are available at all; (4) the audio may contain dis-fluencies and non-speech noises, since they are not specifically recorded for building synthetic voices; and (5) if we obtain automatic transcripts, they will not be error free. Earlier works on long audio alignment addressing the first and second issue generally preferred reasonable transcripts and mainly focused on (1) less manual intervention, (2) mispronunciation detection, and (3) segmentation error recovery. In this work, we use a large vocabulary public domain automatic speech recognition (ASR) system to obtain transcripts, followed by confidence measure-based data pruning which together address the five issues with the found data and also ensure the above three points. For proof of concept, we build voices in the English language using an audiobook (read speech) in a female voice from LibriVox and a lecture (spontaneous speech) in a male voice from Coursera, using both reference and hypotheses transcriptions, and evaluate them in terms of intelligibility and naturalness with the help of a perceptual listening test on the Blizzard 2013 corpus."
75,"
Deep multiple instance learning for foreground speech localization in ambient audio from wearable devices
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Rajat Hebbar, Pavlos Papadopoulos, Ramon Reyes, Alexander F. Danvers, Angelina J. Polsinelli, Suzanne A. Moseley, David A. Sbarra, Matthias R. Mehl and Shrikanth Narayanan",3 February 2021,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-020-00194-0,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-020-00194-0.pdf,"Over the recent years, machine learning techniques have been employed to produce state-of-the-art results in several audio related tasks. The success of these approaches has been largely due to access to large amounts of open-source datasets and enhancement of computational resources. However, a shortcoming of these methods is that they often fail to generalize well to tasks from real life scenarios, due to domain mismatch. One such task is foreground speech detection from wearable audio devices. Several interfering factors such as dynamically varying environmental conditions, including background speakers, TV, or radio audio, render foreground speech detection to be a challenging task. Moreover, obtaining precise moment-to-moment annotations of audio streams for analysis and model training is also time-consuming and costly. In this work, we use multiple instance learning (MIL) to facilitate development of such models using annotations available at a lower time-resolution (coarsely labeled). We show how MIL can be applied to localize foreground speech in coarsely labeled audio and show both bag-level and instance-level results. We also study different pooling methods and how they can be adapted to densely distributed events as observed in our application. Finally, we show improvements using speech activity detection embeddings as features for foreground detection."
76,"
Mapping Speech Spectra from Throat Microphone to Close-Speaking Microphone: A Neural Network Approach
",EURASIP Journal on Advances in Signal Processing,N/A,A. Shahina and B. Yegnanarayana,1 December 2007,https://asp-eurasipjournals.springeropen.com/articles/10.1155/2007/87219,https://asp-eurasipjournals.springeropen.com/counter/pdf/10.1155/2007/87219.pdf,"Speech recorded from a throat microphone is robust to the surrounding noise, but sounds unnatural unlike the speech recorded from a close-speaking microphone. This paper addresses the issue of improving the perceptual quality of the throat microphone speech by mapping the speech spectra from the throat microphone to the close-speaking microphone. A neural network model is used to capture the speaker-dependent functional relationship between the feature vectors (cepstral coefficients) of the two speech signals. A method is proposed to ensure the stability of the all-pole synthesis filter. Objective evaluations indicate the effectiveness of the proposed mapping scheme. The advantage of this method is that the model gives a smooth estimate of the spectra of the close-speaking microphone speech. No distortions are perceived in the reconstructed speech. This mapping technique is also used for bandwidth extension of telephone speech."
77,"
Existence detection and embedding rate estimation of blended speech in covert speech communications
",SpringerPlus,N/A,Lijuan Li and Yong Gao,11 July 2016,https://springerplus.springeropen.com/articles/10.1186/s40064-016-2691-6,https://springerplus.springeropen.com/counter/pdf/10.1186/s40064-016-2691-6.pdf,"Covert speech communications may be used by terrorists to commit crimes through Internet. Steganalysis aims to detect secret information in covert communications to prevent crimes. Herein, based on the average zero crossing rate of the odd–even difference (AZCR-OED), a steganalysis algorithm for blended speech is proposed; it can detect the existence and estimate the embedding rate of blended speech. First, the odd–even difference (OED) of the speech signal is calculated and divided into frames. The average zero crossing rate (ZCR) is calculated for each OED frame, and the minimum average ZCR and AZCR-OED of the entire speech signal are extracted as features. Then, a support vector machine classifier is used to determine whether the speech signal is blended. Finally, a voice activity detection algorithm is applied to determine the hidden location of the secret speech and estimate the embedding rate. The results demonstrate that without attack, the detection accuracy can reach 80 % or more when the embedding rate is greater than 10 %, and the estimated embedding rate is similar to the real value. And when some attacks occur, it can also reach relatively high detection accuracy. The algorithm has high performance in terms of accuracy, effectiveness and robustness."
78,"
Cross-corpus speech emotion recognition using subspace learning and domain adaption
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Xuan Cao, Maoshen Jia, Jiawei Ru and Tun-wen Pai",27 December 2022,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-022-00264-5,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-022-00264-5.pdf,"Speech emotion recognition (SER) is a hot topic in speech signal processing. When the training data and the test data come from different corpus, their feature distributions are different, which leads to the degradation of the recognition performance. Therefore, in order to solve this problem, a cross-corpus speech emotion recognition method is proposed based on subspace learning and domain adaptation in this paper. Specifically, training set data and the test set data are used to form the source domain and target domain, respectively. Then, the Hessian matrix is introduced to obtain the subspace for the extracted features in both source and target domains. In addition, an information entropy-based domain adaption method is introduced to construct the common space. In the common space, the difference between the feature distributions in the source domain and target domain is reduced as much as possible. To evaluate the performance of the proposed method, extensive experiments are conducted on cross-corpus speech emotion recognition. Experimental results show that the proposed method achieves better performance compared with some existing subspace learning and domain adaptation methods."
79,"
Speech steganography using wavelet and Fourier transforms
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Siwar Rekik, Driss Guerchi, Sid-Ahmed Selouani and Habib Hamam",8 August 2012,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/1687-4722-2012-20,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/1687-4722-2012-20.pdf,"A new method to secure speech communication using the discrete wavelet transforms (DWT) and the fast Fourier transform is presented in this article. In the first phase of the hiding technique, we separate the speech high-frequency components from the low-frequency components using the DWT. In a second phase, we exploit the low-pass spectral proprieties of the speech spectrum to hide another secret speech signal in the low-amplitude high-frequency regions of the cover speech signal. The proposed method allows hiding a large amount of secret information while rendering the steganalysis more complex. Experimental results prove the efficiency of the proposed hiding technique since the stego signals are perceptually indistinguishable from the equivalent cover signal, while being able to recover the secret speech message with slight degradation in the quality."
80,"
Query-Driven Strategy for On-the-Fly Term Spotting in Spontaneous Speech
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Mickael Rouvier, Georges Linarès and Benjamin Lecouteux",2 February 2010,https://asmp-eurasipjournals.springeropen.com/articles/10.1155/2010/326578,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1155/2010/326578.pdf,"Spoken utterance retrieval was largely studied in the last decades, with the purpose of indexing large audio databases or of detecting keywords in continuous speech streams. While the indexing of closed corpora can be performed via a batch process, on-line spotting systems have to synchronously detect the targeted spoken utterances. We propose a two-level architecture for on-the-fly term spotting. The first level performs a fast detection of the speech segments that probably contain the targeted utterance. The second level refines the detection on the selected segments, by using a speech recognizer based on a query-driven decoding algorithm. Experiments are conducted on both broadcast and spontaneous speech corpora. We investigate the impact of the spontaneity level on system performance. Results show that our method remains effective even if the recognition rates are significantly degraded by disfluencies."
81,"
Deep encoder/decoder dual-path neural network for speech separation in noisy reverberation environments
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Chunxi Wang, Maoshen Jia and Xinfeng Zhang",12 October 2023,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-023-00307-5,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-023-00307-5.pdf,"In recent years, the speaker-independent, single-channel speech separation problem has made significant progress with the development of deep neural networks (DNNs). However, separating the speech of each interested speaker from an environment that includes the speech of other speakers, background noise, and room reverberation remains challenging. In order to solve this problem, a speech separation method for a noisy reverberation environment is proposed. Firstly, the time-domain end-to-end network structure of a deep encoder/decoder dual-path neural network is introduced in this paper for speech separation. Secondly, to make the model not fall into local optimum during training, a loss function stretched optimal scale-invariant signal-to-noise ratio (SOSISNR) was proposed, inspired by the scale-invariant signal-to-noise ratio (SISNR). At the same time, in order to make the training more appropriate to the human auditory system, the joint loss function is extended based on short-time objective intelligibility (STOI). Thirdly, an alignment operation is proposed to reduce the influence of time delay caused by reverberation on separation performance. Combining the above methods, the subjective and objective evaluation metrics show that this study has better separation performance in complex sound field environments compared to the baseline methods."
82,"
SynFace—Speech-Driven Facial Animation for Virtual Speech-Reading Support
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Giampiero Salvi, Jonas Beskow, Samer Al Moubayed and Björn Granström",16 November 2009,https://asmp-eurasipjournals.springeropen.com/articles/10.1155/2009/191940,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1155/2009/191940.pdf,"This paper describes SynFace, a supportive technology that aims at enhancing audio-based spoken communication in adverse acoustic conditions by providing the missing visual information in the form of an animated talking head. Firstly, we describe the system architecture, consisting of a 3D animated face model controlled from the speech input by a specifically optimised phonetic recogniser. Secondly, we report on speech intelligibility experiments with focus on multilinguality and robustness to audio quality. The system, already available for Swedish, English, and Flemish, was optimised for German and for Swedish wide-band speech quality available in TV, radio, and Internet communication. Lastly, the paper covers experiments with nonverbal motions driven from the speech signal. It is shown that turn-taking gestures can be used to affect the flow of human-human dialogues. We have focused specifically on two categories of cues that may be extracted from the acoustic signal: prominence/emphasis and interactional cues (turn-taking/back-channelling)."
83,"
A parametric prosody coding approach for Mandarin speech using a hierarchical prosodic model
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,Chen-Yu Chiang,11 July 2018,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-018-0129-5,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-018-0129-5.pdf,"In this paper, a novel parametric prosody coding approach for Mandarin speech is proposed. It employs a hierarchical prosodic model (HPM) as a prosody-generating model in the encoder to analyze the speech prosody of the input utterance to obtain a parametric representation of four prosodic-acoustic features of syllable pitch contour, syllable duration, syllable energy level, and syllable-juncture pause duration for encoding. In the decoder, the four prosodic-acoustic features are reconstructed by a synthesis operation using the decoded HPM parameters. The reconstructed prosodic features are lastly used in an HMM-based speech synthesizer to generate the reconstructed speech. Objective and subjective evaluations showed that the proposed prosody coding approach encoded speech with better quality and lower data rate than the conventional segment-based coding scheme with vector or scalar quantization approach did. The reconstructed speech encoded by the proposed approach has good quality at low data rates of 81.4 and 72.7 bps for speaker-dependent and speaker-independent tasks, respectively. An application of the proposed prosody coding approach to speaking rate conversion by directly changing the HPM parameters to those of a different speaking rate is also illustrated. An informal listening test confirmed that both converted speeches of high and low speaking rate sounded very smooth."
84,"
Voice activity detection in the presence of transient based on graph
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Xiao-Yuan Guo, Chun-Xian Gao and Hui Liu",20 April 2023,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-023-00282-x,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-023-00282-x.pdf,"Voice activity detection remains a significant challenge in the presence of transients since transients are more dominant than speech, though it has achieved satisfactory performance in quasi-stationary noisy environments. This paper studies the differences between speech and transients in nonlinear dynamic characteristics and proposes a new method for accurately detecting speech and transients. Limited by algorithm complexity, previous research has proposed few detectors to model speech and transients based on contextual information and thus failing to detect transient frames accurately. To address this challenge, our study proposes to map features of audio signals to a time series complex network, a kind of graph data, analyzed by the Laplacian and adjacency matrix of graphs, then classified by the support vector machine (SVM) classifier. The proposed algorithm can analyze a more extended speech period, allowing the full utilization of contextual information of preceding and following frames. The experimental results show that the performance of this method has obvious superiority over other existing algorithms."
85,"
Structure of pauses in speech in the context of speaker verification and classification of speech type
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Magdalena Igras-Cybulska, Bartosz Ziółko, Piotr Żelasko and Marcin Witkowski",9 November 2016,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-016-0096-7,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-016-0096-7.pdf,"Statistics of pauses appearing in Polish as a potential source of biometry information for automatic speaker recognition were described. The usage of three main types of acoustic pauses (silent, filled and breath pauses) and syntactic pauses (punctuation marks in speech transcripts) was investigated quantitatively in three types of spontaneous speech (presentations, simultaneous interpretation and radio interviews) and read speech (audio books). Selected parameters of pauses extracted for each speaker separately or for speaker groups were examined statistically to verify usefulness of information on pauses for speaker recognition and speaker profile estimation. Quantity and duration of filled pauses, audible breaths, and correlation between the temporal structure of speech and the syntax structure of the spoken language were the features which characterize speakers most. The experiment of using pauses in speaker biometry system (using Universal Background Model and i-vectors) resulted in 30 % equal error rate. Including pause-related features to the baseline Mel-frequency cepstral coefficient system has not significantly improved its performance. In the experiment with automatic recognition of three types of spontaneous speech, we achieved 78 % accuracy, using GMM classifier. Silent pause-related features allowed distinguishing between read and spontaneous speech by extreme gradient boosting with 75 % accuracy."
86,"
A noise PSD estimation algorithm using derivative-based high-pass filter in non-stationary noise conditions
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,Sujan Kumar Roy and Kuldip K. Paliwal,14 August 2021,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-021-00220-9,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-021-00220-9.pdf,"The minimum mean-square error (MMSE)-based noise PSD estimators have been used widely for speech enhancement. However, the MMSE noise PSD estimators assume that the noise signal changes at a slower rate than the speech signal— which lacks the ability to track the highly non-stationary noise sources. Moreover, the performance of the MMSE-based noise PSD estimator largely depends upon the accuracy of the a priori SNR estimation in practice. In this paper, we introduce a noise PSD estimation algorithm using a derivative-based high-pass filter in non-stationary noise conditions. The proposed method processes the silent and speech frames of the noisy speech differently to estimate the noise PSD. It is due to the non-stationary noise that can be mixed with silent and speech-dominated frames non-uniformly. We first introduce a spectral-flatness-based adaptive thresholding technique to detect the speech activity of the noisy speech frames. Since the silent frame of the noisy speech is completely filled with noise, the noise periodogram is directly computed from it without applying any filtering. Conversely, a 4th order derivative-based high-pass filter is applied during speech activity of the noisy speech frame to filter out the clean speech components while leaving behind mostly the noise. The noise periodogram is computed from the filtered signal—which counteracts the leaking of clean speech power. The noise PSD estimate is obtained by recursively averaging the previously estimated noise PSD and the current estimate of the noise periodogram. The proposed method is found to be effective in tracking the rapidly changing as well as the slowly varying noise PSD than the competing methods in non-stationary noise conditions for a wide range of signal-to-noise ratio (SNR) levels. Extensive objective and subjective scores on the NOIZEUS corpus demonstrate that the application of the proposed noise PSD with MMSE-based speech enhancement methods produce higher quality and intelligible enhanced speech than the competing methods."
87,"
A comprehensive system for facial animation of generic 3D head models driven by speech
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Lucas D Terissi, Mauricio Cerda, Juan C Gómez, Nancy Hitschfeld-Kahler and Bernard Girau",1 February 2013,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/1687-4722-2013-5,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/1687-4722-2013-5.pdf,"A comprehensive system for facial animation of generic 3D head models driven by speech is presented in this article. In the training stage, audio-visual information is extracted from audio-visual training data, and then used to compute the parameters of a single joint audio-visual hidden Markov model (AV-HMM). In contrast to most of the methods in the literature, the proposed approach does not require segmentation/classification processing stages of the audio-visual data, avoiding the error propagation related to these procedures. The trained AV-HMM provides a compact representation of the audio-visual data, without the need of phoneme (word) segmentation, which makes it adaptable to different languages. Visual features are estimated from the speech signal based on the inversion of the AV-HMM. The estimated visual speech features are used to animate a simple face model. The animation of a more complex head model is then obtained by automatically mapping the deformation of the simple model to it, using a small number of control points for the interpolation. The proposed algorithm allows the animation of 3D head models of arbitrary complexity through a simple setup procedure. The resulting animation is evaluated in terms of intelligibility of visual speech through perceptual tests, showing a promising performance. The computational complexity of the proposed system is analyzed, showing the feasibility of its real-time implementation."
88,"
Query-by-Example Spoken Term Detection ALBAYZIN 2012 evaluation: overview, systems, results, and discussion
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Javier Tejedor, Doroteo T Toledano, Xavier Anguera, Amparo Varona, Lluís F Hurtado, Antonio Miguel and José Colás",17 September 2013,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/1687-4722-2013-23,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/1687-4722-2013-23.pdf,"Query-by-Example Spoken Term Detection (QbE STD) aims at retrieving data from a speech data repository given an acoustic query containing the term of interest as input. Nowadays, it has been receiving much interest due to the high volume of information stored in audio or audiovisual format. QbE STD differs from automatic speech recognition (ASR) and keyword spotting (KWS)/spoken term detection (STD) since ASR is interested in all the terms/words that appear in the speech signal and KWS/STD relies on a textual transcription of the search term to retrieve the speech data. This paper presents the systems submitted to the ALBAYZIN 2012 QbE STD evaluation held as a part of ALBAYZIN 2012 evaluation campaign within the context of the IberSPEECH 2012 Conferencea. The evaluation consists of retrieving the speech files that contain the input queries, indicating their start and end timestamps within the appropriate speech file. Evaluation is conducted on a Spanish spontaneous speech database containing a set of talks from MAVIR workshopsb, which amount at about 7 h of speech in total. We present the database metric systems submitted along with all results and some discussion. Four different research groups took part in the evaluation. Evaluation results show the difficulty of this task and the limited performance indicates there is still a lot of room for improvement. The best result is achieved by a dynamic time warping-based search over Gaussian posteriorgrams/posterior phoneme probabilities. This paper also compares the systems aiming at establishing the best technique dealing with that difficult task and looking for defining promising directions for this relatively novel task."
89,"
Hybrid statistical/unit-selection Turkish speech synthesis using suffix units
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,Cenk Demiroğlu and Ekrem Güner,2 February 2016,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-016-0082-0,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-016-0082-0.pdf,"Unit selection based text-to-speech synthesis (TTS) has been the dominant TTS approach of the last decade. Despite its success, unit selection approach has its disadvantages. One of the most significant disadvantages is the sudden discontinuities in speech that distract the listeners (Speech Commun 51:1039–1064, 2009). The second disadvantage is that significant expertise and large amounts of data is needed for building a high-quality synthesis system which is costly and time-consuming. The statistical speech synthesis (SSS) approach is a promising alternative synthesis technique. Not only that the spurious errors that are observed in the unit selection system are mostly not observed in SSS but also building voice models is far less expensive and faster compared to the unit selection system. However, the resulting speech is typically not as natural-sounding as speech that is synthesized with a high-quality unit selection system. There are hybrid methods that attempt to take advantage of both SSS and unit selection systems. However, existing hybrid methods still require development of a high-quality unit selection system. Here, we propose a novel hybrid statistical/unit selection system for Turkish that aims at improving the quality of the baseline SSS system by improving the prosodic parameters such as intonation and stress. Commonly occurring suffixes in Turkish are stored in the unit selection database and used in the proposed system. As opposed to existing hybrid systems, the proposed system was developed without building a complete unit selection synthesis system. Therefore, the proposed method can be used without collecting large amounts of data or utilizing substantial expertise or time-consuming tuning that is typically required in building unit selection systems. Listeners preferred the hybrid system over the baseline system in the AB preference tests."
90,"
Search the Audio, Browse the Video—A Generic Paradigm for Video Collections
",EURASIP Journal on Advances in Signal Processing,N/A,"Arnon Amir, Savitha Srinivasan and Alon Efrat",25 February 2003,https://asp-eurasipjournals.springeropen.com/articles/10.1155/S111086570321012X,https://asp-eurasipjournals.springeropen.com/counter/pdf/10.1155/S111086570321012X.pdf,"The amount of digital video being shot, captured, and stored is growing at a rate faster than ever before. The large amount of stored video is not penetrable without efficient video indexing, retrieval, and browsing technology. Most prior work in the field can be roughly categorized into two classes. One class is based on image processing techniques, often called content-based image and video retrieval, in which video frames are indexed and searched for visual content. The other class is based on spoken document retrieval, which relies on automatic speech recognition and text queries. Both approaches have major limitations. In the first approach, semantic queries pose a great challenge, while the second, speech-based approach, does not support efficient video browsing. This paper describes a system where speech is used for efficient searching and visual data for efficient browsing, a combination that takes advantage of both approaches. A fully automatic indexing and retrieval system has been developed and tested. Automated speech recognition and phonetic speech indexing support text-to-speech queries. New browsable views are generated from the original video. A special synchronized browser allows instantaneous, context-preserving switching from one view to another. The system was successfully used to produce searchable-browsable video proceedings for three local conferences."
91,"
Discriminative features based on modified log magnitude spectrum for playback speech detection
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Jichen Yang, Longting Xu, Bo Ren and Yunyun Ji",7 April 2020,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-020-00173-5,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-020-00173-5.pdf,"In order to improve the performance of hand-crafted features to detect playback speech, two discriminative features, constant-Q variance-based octave coefficients and constant-Q mean-based octave coefficients, are proposed for playback speech detection in this work. They rely on our findings that variance-based modified log magnitude spectrum and mean-based modified log magnitude spectrum can enhance the discriminative power between genuine speech and playback speech. Then constant-Q variance-based octave coefficients (constant-Q mean-based octave coefficients) can be obtained by combining variance-based modified log magnitude spectrum (mean-based modified log magnitude spectrum), octave segmentation, and discrete cosine transform. Finally, constant-Q variance-based octave coefficients and constant-Q mean-based octave coefficients are evaluated on ASVspoof 2017 corpus version 2.0 and ASVspoof 2019 physical access, respectively. Experimental results show that variance-based modified log magnitude spectrum and mean-based modified log magnitude spectrum can produce discriminative features toward playback speech. Further results on the two databases show that constant-Q variance-based octave coefficients and constant-Q mean-based octave coefficients can perform better than some common features, such as mel frequency cepstral coefficients and constant-Q cepstral coefficients."
92,"
Enhancement of speech dynamics for voice activity detection using DNN
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Suci Dwijayanti, Kei Yamamori and Masato Miyoshi",12 September 2018,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-018-0135-7,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-018-0135-7.pdf,"Voice activity detection (VAD) is an important preprocessing step for various speech applications to identify speech and non-speech periods in input signals. In this paper, we propose a deep neural network (DNN)-based VAD method for detecting such periods in noisy signals using speech dynamics, which are time-varying speech signals that may be expressed as the first- and second-order derivatives of mel cepstra, also known as the delta and delta-delta features. Unlike these derivatives, in this paper, the dynamics are highlighted by speech period candidates, which are calculated based on heuristic rules for the patterns of the first and second derivatives of the input signals. These candidates, together with the log power spectra, are input into the DNN to obtain VAD decisions. In this study, experiments are conducted to compare the proposed method with a DNN-based method, which exclusively utilizes log power spectra by using speech signals smeared with five types of noise (white, babble, factory, car, and pink) with signal-to-noise ratios (SNRs) of 10, 5, 0, and − 5 dB. The experimental results show that the proposed method is superior under all the considered noise conditions, indicating that the speech period candidates improve the log power spectra."
93,"
A CNN-based approach to identification of degradations in speech signals
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Yuki Saishu, Amir Hossein Poorjam and Mads Græsbøll Christensen",5 February 2021,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-021-00198-4,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-021-00198-4.pdf,"The presence of degradations in speech signals, which causes acoustic mismatch between training and operating conditions, deteriorates the performance of many speech-based systems. A variety of enhancement techniques have been developed to compensate the acoustic mismatch in speech-based applications. To apply these signal enhancement techniques, however, it is necessary to know prior information about the presence and the type of degradations in speech signals. In this paper, we propose a new convolutional neural network (CNN)-based approach to automatically identify the major types of degradations commonly encountered in speech-based applications, namely additive noise, nonlinear distortion, and reverberation. In this approach, a set of parallel CNNs, each detecting a certain degradation type, is applied to the log-mel spectrogram of audio signals. Experimental results using two different speech types, namely pathological voice and normal running speech, show the effectiveness of the proposed method in detecting the presence and the type of degradations in speech signals which outperforms the state-of-the-art method. Using the score weighted class activation mapping, we provide a visual analysis of how the network makes decision for identifying different types of degradation in speech signals by highlighting the regions of the log-mel spectrogram which are more influential to the target degradation."
94,"
Text-independent speaker recognition based on adaptive course learning loss and deep residual network
",EURASIP Journal on Advances in Signal Processing,N/A,"Qinghua Zhong, Ruining Dai, Han Zhang, Yongsheng Zhu and Guofu Zhou",23 July 2021,https://asp-eurasipjournals.springeropen.com/articles/10.1186/s13634-021-00762-2,https://asp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13634-021-00762-2.pdf,"Text-independent speaker recognition is widely used in identity recognition that has a wide spectrum of applications, such as criminal investigation, payment certification, and interest-based customer services. In order to improve the recognition ability of log filter bank feature vectors, a method of text-independent speaker recognition based on deep residual networks model was proposed in this paper. The deep residual network was composed of a residual network (ResNet) and a convolutional attention statistics pooling (CASP) layer. The CASP layer could aggregate frame-level features from the ResNet into an utterance-level features. Extracting speech features for each speaker using deep residual networks was a promising direction to explore, and a straightforward solution was to train the discriminative feature extraction network by using a margin-based loss function. However, a margin-based loss function often has certain limitations, such as the margins between different categories were set to be the same and fixed. Thus, we used an adaptive curriculum learning loss (ACLL) to address the problem and introduce two different margin-based losses for this problem, i.e., AM-Softmax and AAM-Softmax. The proposed method was applied to a large-scale VoxCeleb2 dataset for extensive text-independent speaker recognition experiments, and average equal error rate (EER) could achieve 1.76% on VoxCeleb1 test dataset, 1.91% on VoxCeleb1-E test dataset, and 3.24% on VoxCeleb1-H test dataset. Compared with related speaker recognition methods, EER was improved by 1.11% on VoxCeleb1 test dataset, 1.04% on VoxCeleb1-E test dataset, and 1.69% on VoxCeleb1-H test dataset."
95,"
Automatic Recognition of Lyrics in Singing
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,Annamaria Mesaros and Tuomas Virtanen,23 February 2010,https://asmp-eurasipjournals.springeropen.com/articles/10.1155/2010/546047,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1155/2010/546047.pdf,"The paper considers the task of recognizing phonemes and words from a singing input by using a phonetic hidden Markov model recognizer. The system is targeted to both monophonic singing and singing in polyphonic music. A vocal separation algorithm is applied to separate the singing from polyphonic music. Due to the lack of annotated singing databases, the recognizer is trained using speech and linearly adapted to singing. Global adaptation to singing is found to improve singing recognition performance. Further improvement is obtained by gender-specific adaptation. We also study adaptation with multiple base classes defined by either phonetic or acoustic similarity. We test phoneme-level and word-level n-gram language models. The phoneme language models are trained on the speech database text. The large-vocabulary word-level language model is trained on a database of textual lyrics. Two applications are presented. The recognizer is used to align textual lyrics to vocals in polyphonic music, obtaining an average error of 0.94 seconds for line-level alignment. A query-by-singing retrieval application based on the recognized words is also constructed; in 57% of the cases, the first retrieved song is the correct one."
96,"
Localization based stereo speech source separation using probabilistic time-frequency masking and deep neural networks
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Yang Yu, Wenwu Wang and Peng Han",4 March 2016,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-016-0085-x,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-016-0085-x.pdf,"Time-frequency (T-F) masking is an effective method for stereo speech source separation. However, reliable estimation of the T-F mask from sound mixtures is a challenging task, especially when room reverberations are present in the mixtures. In this paper, we propose a new stereo speech separation system where deep neural networks are used to generate soft T-F mask for separation. More specifically, the deep neural network, which is composed of two sparse autoencoders and a softmax regression, is used to estimate the orientations of the dominant source at each T-F unit, based on low-level features, such as mixing vector (MV), interaural level, and phase difference (IPD/ILD). The dataset for training the networks was generated by the convolution of binaural room impulse responses (RIRs) and clean speech signals positioned in different angles with respect to the sensors. With the training dataset, we use unsupervised learning to extract high-level features from low-level features and use supervised learning to find the nonlinear functions between high-level features and the orientations of dominant source. By using the trained networks, the probability that each T-F unit belongs to different sources (target and interferers) can be estimated based on the localization cues which is further used to generate the soft mask for source separation. Experiments based on real binaural RIRs and TIMIT dataset are provided to show the performance of the proposed system for reverberant speech mixtures, as compared with a model-based T-F masking technique proposed recently."
97,"
Explicit-memory multiresolution adaptive framework for speech and music separation
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Ashwin Bellur, Karan Thakkar and Mounya Elhilali",9 May 2023,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-023-00286-7,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-023-00286-7.pdf,"The human auditory system employs a number of principles to facilitate the selection of perceptually separated streams from a complex sound mixture. The brain leverages multi-scale redundant representations of the input and uses memory (or priors) to guide the selection of a target sound from the input mixture. Moreover, feedback mechanisms refine the memory constructs resulting in further improvement of selectivity of a particular sound object amidst dynamic backgrounds. The present study proposes a unified end-to-end computational framework that mimics these principles for sound source separation applied to both speech and music mixtures. While the problems of speech enhancement and music separation have often been tackled separately due to constraints and specificities of each signal domain, the current work posits that common principles for sound source separation are domain-agnostic. In the proposed scheme, parallel and hierarchical convolutional paths map input mixtures onto redundant but distributed higher-dimensional subspaces and utilize the concept of temporal coherence to gate the selection of embeddings belonging to a target stream abstracted in memory. These explicit memories are further refined through self-feedback from incoming observations in order to improve the system’s selectivity when faced with unknown backgrounds. The model yields stable outcomes of source separation for both speech and music mixtures and demonstrates benefits of explicit memory as a powerful representation of priors that guide information selection from complex inputs."
98,"
ALBAYZIN Query-by-example Spoken Term Detection 2016 evaluation
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Javier Tejedor, Doroteo T. Toledano, Paula Lopez-Otero, Laura Docio-Fernandez, Jorge Proença, Fernando Perdigão, Fernando García-Granada, Emilio Sanchis, Anna Pompili and Alberto Abad",13 April 2018,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-018-0125-9,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-018-0125-9.pdf,"Query-by-example Spoken Term Detection (QbE STD) aims to retrieve data from a speech repository given an acoustic (spoken) query containing the term of interest as the input. This paper presents the systems submitted to the ALBAYZIN QbE STD 2016 Evaluation held as a part of the ALBAYZIN 2016 Evaluation Campaign at the IberSPEECH 2016 conference. Special attention was given to the evaluation design so that a thorough post-analysis of the main results could be carried out. Two different Spanish speech databases, which cover different acoustic and language domains, were used in the evaluation: the MAVIR database, which consists of a set of talks from workshops, and the EPIC database, which consists of a set of European Parliament sessions in Spanish. We present the evaluation design, both databases, the evaluation metric, the systems submitted to the evaluation, the results, and a thorough analysis and discussion. Four different research groups participated in the evaluation, and a total of eight template matching-based systems were submitted. We compare the systems submitted to the evaluation and make an in-depth analysis based on some properties of the spoken queries, such as query length, single-word/multi-word queries, and in-language/out-of-language queries."
99,"
Residual feedback suppression with extended model-based postfilters
","EURASIP Journal on Audio, Speech, and Music Processing",N/A,"Marco Gimm, Philipp Bulling and Gerhard Schmidt",28 May 2021,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-021-00205-8,https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-021-00205-8.pdf,"When designing closed-loop electro-acoustic systems, which can commonly be found in hearing aids or public address systems, the most challenging task is canceling and/or suppressing the feedback caused by the acoustic coupling of the transducers of such systems. In many applications, feedback cancelation based on adaptive filters is used for this purpose. However, due to computational complexity such a feedback canceler is often limited in the length of the filter’s impulse response. Consequently, a residual feedback, which is still audible and may lead to system instability, remains in most cases. In this work, we present enhancements for model-based postfilters based on a priori knowledge of the feedback path which can be used cooperatively with the adaptive filter-based feedback cancelation system to suppress residual feedback and improve the overall feedback reduction capability. For this, we adapted an existing reverberation model such that our model additionally considers the presence and the performance of the adaptive filter. We tested the effectiveness of our approach by means of both objective and subjective evaluations."
100,"
Face mask type affects audiovisual speech intelligibility and subjective listening effort in young and older adults
",Cognitive Research: Principles and Implications,N/A,"Violet A. Brown, Kristin J. Van Engen and Jonathan E. Peelle",18 July 2021,https://cognitiveresearchjournal.springeropen.com/articles/10.1186/s41235-021-00314-0,https://cognitiveresearchjournal.springeropen.com/counter/pdf/10.1186/s41235-021-00314-0.pdf,"Identifying speech requires that listeners make rapid use of fine-grained acoustic cues—a process that is facilitated by being able to see the talker’s face. Face masks present a challenge to this process because they can both alter acoustic information and conceal the talker’s mouth. Here, we investigated the degree to which different types of face masks and noise levels affect speech intelligibility and subjective listening effort for young (N = 180) and older (N = 180) adult listeners. We found that in quiet, mask type had little influence on speech intelligibility relative to speech produced without a mask for both young and older adults. However, with the addition of moderate (− 5 dB SNR) and high (− 9 dB SNR) levels of background noise, intelligibility dropped substantially for all types of face masks in both age groups. Across noise levels, transparent face masks and cloth face masks with filters impaired performance the most, and surgical face masks had the smallest influence on intelligibility. Participants also rated speech produced with a face mask as more effortful than unmasked speech, particularly in background noise. Although young and older adults were similarly affected by face masks and noise in terms of intelligibility and subjective listening effort, older adults showed poorer intelligibility overall and rated the speech as more effortful to process relative to young adults. This research will help individuals make more informed decisions about which types of masks to wear in various communicative settings."
