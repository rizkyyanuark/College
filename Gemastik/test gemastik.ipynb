{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import words\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag, pos_tag_sents\n",
    "\n",
    "#import for bag of word\n",
    "import numpy as np\n",
    "#For the regular expression\n",
    "import re\n",
    "#Textblob dependency\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "#set to string \\\n",
    "from ast import literal_eval\n",
    "#From src dependency \n",
    "# from sentencecounter import no_sentences,getline,gettempwords \n",
    "\n",
    "import os\n",
    "def getsysets(word):\n",
    "    syns = wordnet.synsets(word)  #wordnet from ntlk.corpus  will not work with textblob\n",
    "    #print(syns[0].name()) \n",
    "    #print(syns[0].lemmas()[0].name())  #get synsets names \n",
    "    #print(syns[0].definition())  #defination \n",
    "    #print(syns[0].examples())    #example\n",
    "\n",
    "\n",
    "# getsysets(\"good\")\n",
    "\n",
    "\n",
    "def getsynonyms(word):\n",
    "    synonyms = []\n",
    "    # antonyms = []\n",
    " \n",
    "    for syn in wordnet.synsets(word):\n",
    "        for l in syn.lemmas():\n",
    "            synonyms.append(l.name())\n",
    "            # if l.antonyms():\n",
    "            #     antonyms.append(l.antonyms()[0].name())\n",
    " \n",
    "    # print(set(synonyms))\n",
    "    return(set(synonyms))\n",
    "    # print(set(antonyms))\n",
    "\n",
    "\n",
    "# getsynonyms_and_antonyms(\"good\")\n",
    "\n",
    "\n",
    "def extract_words(sentence):\n",
    "    ignore_words = ['a']\n",
    "    words = re.sub(\"[^\\w]\", \" \",  sentence).split() #nltk.word_tokenize(sentence)\n",
    "    words_cleaned = [w.lower() for w in words if w not in ignore_words]\n",
    "    return words_cleaned    \n",
    "\n",
    "\n",
    "def tokenize_sentences(sentences):\n",
    "    words = []\n",
    "    for sentence in sentences:\n",
    "        w = extract_words(sentence)\n",
    "        words.extend(w)\n",
    "        \n",
    "    words = sorted(list(set(words)))\n",
    "    return words\n",
    "\n",
    "def bagofwords(sentence, words):\n",
    "    sentence_words = extract_words(sentence)\n",
    "    # frequency word count\n",
    "    bag = np.zeros(len(words))\n",
    "    for sw in sentence_words:\n",
    "        for i,word in enumerate(words):\n",
    "            if word == sw: \n",
    "                bag[i] += 1\n",
    "                \n",
    "    return np.array(bag)\n",
    "\n",
    "def tokenizer(sentences):\n",
    "    token = word_tokenize(sentences)\n",
    "    return token\n",
    "    print(\"#\"*100)\n",
    "    print (sent_tokenize(sentences))\n",
    "    print (token)\n",
    "    print(\"#\"*100)\n",
    "\n",
    "\n",
    "# sentences = \"Machine learning is great\",\"Natural Language Processing is a complex field\",\"Natural Language Processing is used in machine learning\"\n",
    "# vocabulary = tokenize_sentences(sentences)\n",
    "# print (vocabulary)\n",
    "# tokenizer(sentences)\n",
    "\n",
    "def createposfile(filename,word):\n",
    "    # filename = input(\"Enter destination file name in string format :\")\n",
    "    f = open(filename,'w')\n",
    "    f.writelines(word+'\\n')\n",
    "\n",
    "def createnegfile(filename,word):\n",
    "    # filename = input(\"Enter destination file name in string format :\")\n",
    "    f = open(filename,'w')\n",
    "    f.writelines(word)\n",
    "\n",
    "def getsortedsynonyms(word):\n",
    "    sortedsynonyms = sorted(getsynonyms(word))\n",
    "    return sortedsynonyms\n",
    "\n",
    "def getlengthofarray(word):\n",
    "    return getsortedsynonyms(word).__len__()\n",
    "\n",
    "def readposfile():\n",
    "    f = open('list of positive words.txt')\n",
    "    return f\n",
    "\n",
    "# def searchword(word, sourcename):\n",
    "#     if word in open('list of negative words.txt').read():\n",
    "#             createnegfile('destinationposfile.txt',word)\n",
    "#     elif word in open('list of positive words.txt').read():\n",
    "#             createposfile('destinationnegfile.txt',word)     \n",
    "\n",
    "#     else:\n",
    "#         for i in range (0,getlengthofarray(word)):\n",
    "#             searchword(getsortedsynonyms(word)[i],sourcename)\n",
    "\n",
    "def searchword(word,srcfile):\n",
    "    # if word in open('list of negative words.txt').read():\n",
    "    #         createnegfile('destinationposfile.txt',word)\n",
    "    if word in open('list of positive words.txt').read():\n",
    "            createposfile('destinationnegfile.txt',word)\n",
    "    else:\n",
    "        for i in range(0,getlengthofarray(word)):\n",
    "            searchword(sorted(getsynonyms(word))[i],srcfile)\n",
    "            f = open(srcfile,'w')\n",
    "            f.writelines(word)\n",
    "\n",
    "print ('#'*50)\n",
    "# searchword('lol','a.txt')\n",
    "print(readposfile())\n",
    "# tokenizer(sentences)\n",
    "# getsynonyms('good')\n",
    "# print(sorted(getsynonyms('good'))[2])  #finding an array object [hear it's 3rd object]\n",
    "print ('#'*50)\n",
    "# print (getsortedsynonyms('bad').__len__())\n",
    "# createposfile('created.txt','lol')\n",
    "# for word in word_tokenize(getline()):\n",
    "#     searchword(word,'a.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
